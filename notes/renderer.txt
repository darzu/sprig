WebGL
  https://twgljs.org // tiny layer over webgl
  https://twgljs.org/examples/twgl-cube.html
  https://twgljs.org/examples/webgl-cube.html
  https://github.com/pmndrs/react-three-fiber
  https://enable3d.io (three.js, ammo.js, capacitor.js, phaser, )
  https://github.com/tamani-coding/enable3d-physics-examples

Three.js:
  https://threejs.org
  https://www.npmtrends.com/babylonjs-vs-three


WebGL learning:
  https://games.greggman.com/game/webgl-3d-cameras/
  https://webglfundamentals.org/webgl/lessons/webgl-3d-camera.html
  https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API/WebGL_best_practices
  https://www.tutorialspoint.com/webgl/webgl_interactive_cube.htm

Babylon:
  https://www.babylonjs.com
  https://playground.babylonjs.com
  https://nme.babylonjs.com // material editor (flow based)
  https://github.com/BabylonJS/SummerFestival
  https://doc.babylonjs.com/guidedLearning/createAGame
  https://babylonjs.medium.com/from-unity-to-babylon-js-how-is-the-journey-c71f79482aa3
  https://news.ycombinator.com/from?site=babylonjs.com

Three.js tutorial:
  https://www.youtube.com/watch?v=cp-H_6VODko

HAXE:
  http://babylonhx.com
  https://haxe.org/use-cases/games/
    Northgard, Dead Cells, "Papers, Please", Rymdkapsel
  https://github.com/armory3d

  Heaps.io:
    https://heaps.io

Multi-platform:
  https://github.com/expo/expo/tree/master/packages/expo-gl#expo-gl

PlayCanvas:
  https://playcanvas.com
  praise: https://news.ycombinator.com/item?id=27050731
  https://news.ycombinator.com/from?site=playcanvas.com
  https://blog.playcanvas.com/a-multiplayer-3rd-person-shooter-in-html5/

Unity Tiny:
  https://unity.com/solutions/instant-games
  https://forum.unity.com/threads/project-tiny-0-32-preview-is-available-ui-new-skinned-mesh-renderer-blendshape-sample.1045204/
  https://tiny.vision/demos/Tiny3D/Wasm/Tiny3D.html (needs Chrome or Safari Preview)
  https://github.com/Unity-Technologies/ProjectTinySamples/tree/master/Tiny3D
  (OLD!) https://docs.unity3d.com/Packages/com.unity.tiny@0.13/manual/scripting-systems.html
  (OLD!) https://docs.unity3d.com/Packages/com.unity.tiny@0.13/manual/intro-for-unity-developers.html

Pict3D:
  https://docs.racket-lang.org/pict3d/index.html

Regl:
  https://github.com/regl-project/regl

e.g.
  https://github.com/jacklaplante/bowdown
  https://github.com/onegeek/webglu

Sketch fab:
  https://sketchfab.com

Defold:
  https://defold.com
    "I recall our unity engineer quite liked Defold, which had an 
    integrated builder tool, but for some reason the lead dev didn’t 
    want to go with it." - https://news.ycombinator.com/item?id=24018097
  https://github.com/defold/defold
  Owned by King (Candy Crush, etc)

Google Docs switching to canvas:
  https://workspaceupdates.googleblog.com/2021/05/Google-Docs-Canvas-Based-Rendering-Update.html

Flutter does all UI via canvas:
  https://gallery.flutter.dev/#/

Construct:
  https://www.construct.net/en/make-games/showcase

WebGPU
    https://github.com/gpuweb/gpuweb
    status:
        https://github.com/gpuweb/gpuweb/wiki/Implementation-Status
    spec:
        https://gpuweb.github.io/gpuweb/
    examples:
      https://www.willusher.io/projects#WebGPU%20Experiments

Construct blog:
  https://www.construct.net/en/blogs/ashleys-blog-2/brief-history-graphics-web-1517
  https://www.construct.net/en/blogs/ashleys-blog-2/webgl-webgpu-construct-1519

Safari WebGPU intro:
  https://webkit.org/blog/9528/webgpu-and-wsl-in-safari/

Mozilla WebGPU intro:
  https://hacks.mozilla.org/2020/04/experimental-webgpu-in-firefox/

Chrome WebGPU intro:
  https://www.youtube.com/watch?v=K2JzIUIHIhc
  
TO DECIDE:
    Build our own renderer?

Understand URP:
    https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@12.0/manual/universalrp-builtin-feature-comparison.html'

GLTF viewers:
    https://sandbox.babylonjs.com/
    https://gltf-viewer.donmccurdy.com/
    https://gltf.insimo.com/
    https://github.khronos.org/glTF-Sample-Viewer-Release/
        https://github.com/KhronosGroup/glTF-Sample-Viewer

GLTF loader:
    https://github.com/mrdoob/three.js/blob/a98b9bf/examples/js/loaders/GLTFLoader.js

GLTF compression:
    https://google.github.io/draco/
        has JS encode / decode libraries

WebGPU on Chrome:
    https://developers.google.com/web/updates/2019/08/get-started-with-gpu-compute-on-the-web

WebGPU tutorials:
    https://alain.xyz/blog/raw-webgpu
        matrix library: https://github.com/toji/gl-matrix
    https://www.willusher.io/graphics/2020/06/15/0-to-gltf-triangle

WebGPU samples:
    https://github.com/mikbry/awesome-webgpu
    http://austin-eng.com/webgpu-samples/samples/computeBoids

Vulkan vs OpenGL:
    https://gamedev.stackexchange.com/questions/96014/what-is-vulkan-and-how-does-it-differ-from-opengl

WebGPU uses Web Shading Language (WSL)

webgpu-rs:
    https://github.com/gfx-rs/wgpu-rs
    Why WebGPU for native is good:
      http://kvark.github.io/web/gpu/native/2020/05/03/point-of-webgpu-native.html
    https://dawn.googlesource.com/dawn (webgpu on native by Google)

MIT computer graphics lectures:
    https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-837-computer-graphics-fall-2012/lecture-notes/
    lectures
      https://www.youtube.com/watch?v=-LqUu61oRdk&list=PLQ3UicqQtfNuBjzJ-KEWmG1yjiRMXYKhh
      https://www.youtube.com/watch?v=t7g2oaNs-c8&list=PLQ3UicqQtfNuKZjdA3fY1_X9gXn13JLlW

Game Engine Architecture book:
    https://www.gameenginebook.com/

RTX / ray tracing:
  minecraft RTX: https://alain.xyz/blog/frame-analysis-minecraftrtx
  on m1 https://www.willusher.io/graphics/2020/12/20/rt-dive-m1
    ~7-9 million rays / sec
  beyond ray-tracing http://sci.utah.edu/~will/papers/rtx-points-hpg19.pdf
  on GPU via Cuda: https://developer.nvidia.com/blog/accelerated-ray-tracing-cuda/
  Ray Tracing Gems II:
    http://www.realtimerendering.com/raytracinggems/rtg2/index.html

pbr / physically based rendering:
  https://pbrt.org
  https://www.pbr-book.org/3ed-2018/contents

GPU API concepts compared:
  https://alain.xyz/blog/comparison-of-modern-graphics-apis

Defered rendering:
  https://gamedev.stackexchange.com/questions/74/what-is-deferred-rendering
    Forward: O(geometry * lights)
    Defered: O(geometry + lights)
  Works poorly for transparency (most engines use Forward)
    see: depth peeling
  Uses large amounts of VRAM and frame buffer bandwidth
  stencil-based geometry vs "tiled/froxel compute-based shading"
  Unity: https://docs.unity3d.com/2021.2/Documentation/Manual/RenderTech-DeferredShading.html
  https://www.reddit.com/r/gamedev/comments/8klygv/is_deferred_shading_still_considered_state_of_the/
    "It looks like the industry is going towards a forward+/hybrid approach"
      0. Render Depth Prepass (optional, could prepare a thin gbuffer on this pass)
      1. Use depth buffer (or not) to bin lights into screenspace tiles, save this as a light buffer
      2. Render the geometry with the full shader, and get the light information from the screenspace coordinate, wich lets you see the lights on that tile
      3. Postprocess.
    "Forward+ splits the screen into a 2d grid and a process (compute or other shader) figures out what lights affect that tile. Forward++ takes this a step 
    further and instead of a 2d grid splitting the screen, its a 3d grid splitting space with perspective."
  used by Horizon Zero Dawn

Shadows:
  Shadow volumes? (aka stencil shadows)
    https://en.wikipedia.org/wiki/Shadow_volume
  Shadow mapping
    https://www.youtube.com/watch?v=o6zDfDkOFIc
    use the normal view frustum to compute the bounding box that should be used for the shadow map
      compute view frustum with different far-clipping plane to adjust shadow distance

Lighting:
  pixel vs vertex
    could subdivide large triangles so vertex lighting is more accurate; no storage cost
    vertex + tessellation for particles (e.g. smoke):
      http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.187&rep=rep1&type=pdf
  Spherical Harmonics lighting
    https://computergraphics.stackexchange.com/questions/4164/what-are-spherical-harmonics-light-probes
    https://mynameismjp.wordpress.com/2016/10/09/sg-series-part-1-a-brief-and-incomplete-history-of-baked-lighting-representations/
  deferred vs forward
  https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/shading-normals
  WebGPU "clustered forward shading"
    https://toji.github.io/webgpu-clustered-shading/
    https://github.com/toji/webgpu-clustered-shading
      "I don't think WGSL has atomic methods yet so I can't effectively do the light list compacting 
      the way I want (or at least I don't know the workaround)."

TO LEARN:
  Fourier transform
    then Spherical Harmonics

GPU perf for artists:
  http://www.fragmentbuffer.com/gpu-performance-for-game-artists/
    https://news.ycombinator.com/item?id=14726355
    https://news.ycombinator.com/item?id=21978146

Frame analysis:
  compilation:
    http://www.adriancourreges.com/blog/
  RenderDoc:
    https://renderdoc.org
    https://spector.babylonjs.com (for WebGL)
  https://zhangdoa.com/posts/rendering-analysis-cyberpunk-2077
  https://alain.xyz/blog/frame-analysis-overwatch
  https://alain.xyz/blog/frame-analysis-mk11
  https://alain.xyz/blog/frame-analysis-minecraftrtx
  https://aschrein.github.io/2019/08/11/metro_breakdown.html
  https://aschrein.github.io/2019/08/01/re2_breakdown.html
  http://www.adriancourreges.com/blog/2016/09/09/doom-2016-graphics-study/
    https://news.ycombinator.com/item?id=12461896
    http://advances.realtimerendering.com/s2016/Siggraph2016_idTech6.pdf
  http://www.adriancourreges.com/blog/2017/12/15/mgs-v-graphics-study/
  http://www.adriancourreges.com/blog/2015/11/02/gta-v-graphics-study/
    https://news.ycombinator.com/item?id=10492876
  http://www.adriancourreges.com/blog/2015/06/23/supreme-commander-graphics-study/
    https://news.ycombinator.com/item?id=9770020
  http://www.adriancourreges.com/blog/2015/03/10/deus-ex-human-revolution-graphics-study/
    https://news.ycombinator.com/item?id=9565891

BLOGS TO SCAN:
  https://www.ea.com/frostbite/news
  http://advances.realtimerendering.com/s2015/index.html (siggraph conference)
  https://simonschreibt.de/game-art-tricks/
  https://www.willusher.io
    lots of graphics projects: https://www.willusher.io/projects
    teapot rendering challenge https://graphics.cs.utah.edu/trc/
  https://www.cs.washington.edu/research/graphics
    https://grail.cs.washington.edu/research/
    http://courses.cs.washington.edu/courses/cse457/ Computer Graphics
    http://courses.cs.washington.edu/courses/cse458/ Computer Animation
    http://courses.cs.washington.edu/courses/csep557/ Trends in Computer Graphics
    http://courses.cs.washington.edu/courses/cse557/ Computer Graphics
  http://madebyevan.com

coloring models:
  https://www.willusher.io/webgl-ewa-splatter/#Dinosaur

TO READ:
  https://forum.beyond3d.com/threads/gpu-driven-rendering-siggraph-2015-follow-up.57240/
    "about GPU-driven rendering pipelines"

GDC:
  Horizon Zero Dawn:
    Vegitation https://www.youtube.com/watch?v=wavnKZNSYqU
    game design https://www.youtube.com/watch?v=TawhcWao9ls
    procedural gen https://www.youtube.com/watch?v=ToCozpl1sYY

Rust -> WebGPU examples:
  https://github.com/gfx-rs/wgpu-rs/tree/master/examples
  https://wgpu.rs

"Mixed resolution rendering":
  https://www.gdcvault.com/play/1022982/Mixed-Resolution-Rendering-in-Skylanders
    for clouds based on sprites
    downsample depth -> raster -> "bilateral" upsample 

LOD / automatic mesh simplification:
  quad simplification: https://www.youtube.com/watch?v=vBJcdClynFE

Rendering pipeline:
  https://www.khronos.org/opengl/wiki/Rendering_Pipeline_Overview

hierarchical /  models:
  https://sites.google.com/site/csc8820/educational/how-to-animate-hierarchical-models
  https://canvas.dartmouth.edu/courses/16840/assignments/82764

how many triangles?
  "overwatch ingame characters has 60K tris"

"bundles"?
  https://computergraphics.stackexchange.com/questions/4066/whats-the-main-difference-of-pipeline-process-between-vulkan-and-dx12

Why r draw calls expensive?
  "because if you send too little to the GPU, you're CPU bound and the GPU idles"
  https://stackoverflow.com/questions/4853856/why-are-draw-calls-expensive
  https://www.nvidia.com/docs/IO/8228/BatchBatchBatch.pdf

Data per triangle:
  "instanced vertex attributes"
  gl_PrimitiveID
  "texture buffer object (TBO)" for high primitive count
    texelFetch

Graphics storage options:
  Vertex buffer
  Vertex buffer (instance step)
  Uniform buffer
    Small, performant
  SSBOS
    large, slow, writable

Unreal 5 analysis:
  https://www.elopezr.com/a-macro-view-of-nanite/

Camera transformations:
  https://www.3dgep.com/understanding-the-view-matrix/#Transformations
  http://www.codinglabs.net/article_world_view_projection_matrix.aspx
  https://gamedev.stackexchange.com/questions/178643/the-view-matrix-finally-explained
  https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix
  http://web.cse.ohio-state.edu/~wang.3602/courses/cse5542-2013-spring/6-Transformation_II.pdf
  https://learnopengl.com/Getting-started/Coordinate-Systems

hierarchical modeling:
  https://www.youtube.com/watch?v=JdhpViedm0g&list=PLQ3UicqQtfNuBjzJ-KEWmG1yjiRMXYKhh&index=5

Low-poly planet map effect:
  https://www.youtube.com/watch?v=i5zwDoYXH5c

WebGPU vs WebGL:
  https://www.babylonjs.com/demos/webgpu/forestwebgl
  https://www.babylonjs.com/demos/webgpu/forestwebgpu

Alpha tested vs transparency:
  https://forum.unity.com/threads/difference-between-alphatest-and-transparent-renderqueue.458750/
  alpha tested renders front-to-back (occluded pixels r ignored)
  transparency renders back-to-front

transparency:
  http://learnwebgl.brown37.net/11_advanced_rendering/alpha_blending.html
    in the standard technique, u have to order transparent objects back to front and
      render them in a seperate pass
    b/c alpha blending is not commutitive in real life; 
      a 99% opaque object in front will dominate the result color
    unless: 
      only one transparent object; 
      transparent objects never overlap in the camera;
    use insertion sort b/c less work when nothing changes frame to frame

"order independent transparency":
  https://interplayoflight.wordpress.com/2022/06/25/order-independent-transparency-part-1/
  https://interplayoflight.wordpress.com/2022/07/02/order-independent-transparency-part-2/

Texture compression:
  BC7, BC4, BCn
  https://www.reedbeta.com/blog/understanding-bcn-texture-compression-formats/

Foveated rendering:
  https://en.wikipedia.org/wiki/Foveated_rendering

GPU hardware:
  Rasterization is still done in fixed hardware, not programmable
    https://en.wikipedia.org/wiki/Rasterisation
    (Intel tried with Larrabee, aborted)
    (Nvidia tried: https://highperformancegraphics.net/previous/www_2011/media/Papers/HPG2011_Papers_Laine.pdf, 
      https://dl.acm.org/doi/10.1145/2018323.2018337, http://code.google.com/p/cudaraster/)
      Issues: interpolation, anti-aliasing, power consumption
    Desire for software rasterization:
      shader perf boost?
      ROP (render output unit): a-buffering, order-independent transparency
      Stochastic rasterization
      Non-linear rasterization
  Vertex and fragment shaders
  Compute vs Fragment for post-processing:
    https://computergraphics.stackexchange.com/questions/54/when-is-a-compute-shader-more-efficient-than-a-pixel-shader-for-image-filtering
  ROP:
    https://en.wikipedia.org/wiki/Render_output_unit
    handles anti-aliasing (e.g. MSAA)

Z-order curve:
  https://en.wikipedia.org/wiki/Z-order_curve#Applications
  Zig-zag textures for more efficient cache locality

Albedo vs Diffuse:
  https://computergraphics.stackexchange.com/questions/350/albedo-vs-diffuse

Virtual texturing:
  http://holger.dammertz.org/stuff/notes_VirtualTexturing.html
  https://computergraphics.stackexchange.com/questions/1768/how-can-virtual-texturing-actually-be-efficient

Don't have T junctions in meshes:
  https://computergraphics.stackexchange.com/questions/1461/why-do-t-junctions-in-meshes-result-in-cracks

Frustum culling:
  https://www.iquilezles.org/www/articles/frustumcorrect/frustumcorrect.htm

ALTERNATE APPROACH:
  It could be worth considering going for a low-res ray tracing. E.g. maybe just 256x256 pixels, but 
  ray tracing them all.
  This would let us drastically simplify the renderer and do the cool RTX techniques
  Using SDF tech, we could get some very cool meshes and mesh building

SDF for text:
  https://steamcdn-a.akamaihd.net/apps/valve/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf
  https://www.youtube.com/watch?v=1b5hIMqz_wM
    Material Maker
  text is hard?
    https://gankra.github.io/blah/text-hates-you/
  https://www.qt.io/blog/2011/07/15/text-rendering-in-the-qml-scene-graph
  https://computergraphics.stackexchange.com/questions/306/sharp-corners-with-signed-distance-fields-fonts

VFX explosions:
  https://www.youtube.com/watch?v=dDsb_9n-Gik

Flow maps and gradient maps for moving textures:
  https://www.youtube.com/watch?v=KfphtLRoUB0

Jump Flood Algorithm:
  Ben: https://bgolus.medium.com/the-quest-for-very-wide-outlines-ba82ed442cd9
  https://prideout.net/blog/distance_fields/
  https://blog.demofox.org/2016/02/29/fast-voronoi-diagrams-and-distance-dield-textures-on-the-gpu-with-the-jump-flooding-algorithm/
  https://shaderbits.com/blog/various-distance-field-generation-techniques
    https://www.shadertoy.com/view/4syGWK
  Freya: https://twitter.com/freyaholmer/status/1292536599230775296?lang=en
  https://www.youtube.com/watch?v=A0pxY9QsgJE

  can u use JFA to blur stars instead of gauss blur? easier to make arbitrarily big,
    maybe easier to modulate based on distance to camera

  "Triangle Voronoi Borders":
    https://www.shadertoy.com/view/ss3fW4

  Related to voronoi?
    https://www.redblobgames.com/x/2022-voronoi-maps-tutorial/
    Delaunator: https://github.com/mapbox/delaunator (good license)
      guide: https://mapbox.github.io/delaunator/
    Delaunator seems good for CPU and gives a mesh; unclear about GPU perf.

  
Outlines
  https://bgolus.medium.com/the-quest-for-very-wide-outlines-ba82ed442cd9
  overwatch and control use "sobol" for outlines?
    https://www.youtube.com/watch?v=hxPQ2F96F9E
    looks like a sampling technique:
      https://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction/Sobol_Sampler

Mesh simplification:
  https://www.simplygon.com
  techniques: billboard cloud, flip book

Noise is good in game art?:
  makes things less flat
  https://simblob.blogspot.com/2009/06/noise-in-game-art.html

Factory Station's rendering:
  https://twitter.com/GravitonPunch/status/1453072441316675584
  https://twitter.com/alexanderameye/status/1375463146446585857

Rendering problem cheat sheet:
  https://techartaid.com/cheatsheet/

To improve look and feel, I should write a .gltf exporter than load that in Unity 
  or blender and play with materials, lighting, shaders etc until it looks how I want.
  Then figure out what the delta is and how to impl it. Probably cut corners.

Waves:
  https://playground.babylonjs.com/?webgpu#YX6IB8#55
  https://github.com/gasgiant/FFT-Ocean

WebGPU breakout sample:
  https://github.com/toji/spookyball

Tomas Sala doesn't believe in textures either:
  https://youtu.be/5d8tx6K6hkk?t=3050
  no baking lights
  no diffuse maps, no normal maps,
  world-space fog gradients use extensively
  world-space aware shaders in general
  day-night cycle:
    one slider, indexes ~8 gradients
  having no textures means he has tons of memory:
    his audio is uncompressed
    his whole world is loaded in at once
    consistent 60fps without much effort
  doesn't do any special culling, just frustum culling built into Unity
  regarding low-poly:
    he doesn't like showing all the facetted edges,
    "smoothing groups" ?
    some vertices smooth together, some stay sharp with each other
  b/c models are "kitbashed", destruction just means flinging individual parts
  hmm seems he does bake some lights into vertex colors
  uses forward rendering for perf reasons
  for flood light: just use a box w/ additive color
  clouds: transparent spheres, z sorted ?
    ray marched, volumetric stuff is too expensive
  "i'm not smart enough to use that so I won't"
  "find the pillars you need and focus on those"
    In finding nemo, they had a small list of things that make ocean water work
    in the outdoor setting of falconeer, 
  "the smaller the prison the more exciting the break out"
    limitations breed creative solutions
  color correction: "that is so important"
  things need to feel consistent
    water color reflected in sky
    things need to feel solid, like they r one piece
  in unity the perf metrics: batches and tris
    <200 batches, <200,000 triangles for mobile
    <400 batches, <400,000 triangles for switch
      up to 600 batches, 600,000 triangles
    on PC, 1,000 batches, 1,000,000 triangles is safe
  ocean 
    is "occlusion shape" ? only rendered for camera
    ~50k triangles
    ocean shader done in Shader Forge
      omg, it's huge
    waves itself: vertex displacement, "gerstner wave" ?
      sine wave, absolute value, but soft tops?
      7 iterations, bigger and smaller, merged
      has mirror code in C# and shader for displacement
    has shadows on the water
    never got ambient occlusion to work on the water
    occasionally generate wave splaces
  for reflections:
    box blur is too expensive
    uses lots of jagged sine waves?
    planar reflection
  lip sync by just have emotional blabber blended to 
    mouth closed based on audio
  clouds:
    inspiration from dutch moody sky ship paintings
    "I'm not smart enough to understand how ray marched volumetric clouds really work"
    Ace Combat has blog on their clouds (too fancy?)
      https://www.skywardfm.com/post/the-form-and-function-of-clouds-and-weather-in-ace-combat-7
    Tomas vid on clouds: https://www.youtube.com/watch?v=4i3w9XmxYSo
      spheres
      fresnel + noise for soft outer transparency
      lighting underneath (eventually removed?)
        prevented b/c of "tri planar mapping"
        noise is world-space dependent
        light based on direction and sky gradient
      not particles, needed custom sorting
      sorting issues are less of problem b/c of tri planar mapping ?
        b/c clouds are so similar to their neighbors, u don't notice clipping/sorting
      only heavy b/c of alpha sorting
      sorting orthographicly on switch, "down axis"
    some high up clouds 
    global "light color" is shared between many many shaders
  skybox is a sphere
  combination of tricks makes it work:
    sphere clouds, sphere skybox, day/night lighting, water shader, 
    water reflections, water shadows, ground fog, 
    short render distance makes it feel more atmospheric
  ocean big displacements:
    giant ocean trench:
      displace waves based on worldspace z coordinate sine wave
      acquaduct bridge crossing the canyon
    sphere displacement, can even animate
    caves that are just clipping the ocean
    b/c displacement is world space, it's trivial to create another 
      plane using the same displacements and just compose them
      https://youtu.be/5d8tx6K6hkk?t=8103
  birds flap forward?
  for 120hz, only the bird and camera are frame rate independent
  uses scriptable objects to create missions
    would have loved to know unity editor scripting better
  made the game front to back: don't do that, b/c ur best work will be
    at the end. u want ur peak work and tools for the first bit
    people r not patient
  more re textureless: https://www.youtube.com/watch?v=Bu5mxNyR8uA
    the less u do with textures, the more u do with colors, interesting things start to happen
    about achieving an emotional effect with the least amount of detail
    spent a bunch of time on the pipeline (shaders etc) so that simple shapes go in and come out looking good
    for geometry, only two shaders: one glowy, one not
  another vid: https://www.youtube.com/watch?v=UhtkcRyGG6o
    just avoids adding things that he knows are hard and will take him out of the flow
    chasing certain bugs will just give writers block for a week
    uses "smoothing groups"
    UV coords, just make a gradient

Death's door uses fairly minimal textures:
  https://youtu.be/pcSmBGkbd-g?list=PLX2vGYjWbI0S44qONl7OmB5tpq1YaFN8F&t=3197

Font choices:
  Montserrat ? https://twitter.com/patriciogv/status/1526539963743059968

PBR:
  https://www.youtube.com/watch?v=RRE-F57fbXw
  common traits:
    energy conservation
    microfacet model
    fresnel effect
  rendering equation:
    L0(x,wo,l,t) = Le(x,wo,l,t) + INT_O(fr(x,wi,wo,l,t)Li(x,wi,l,t)(wi*n)dwi)
      x = frament position
      wo = outgoing light
      wi = negative incoming light
      l = wavelength
      t = time
    L0(x,V) = Le(x,V) + INT_O(fr(x,L,V)Li(x,L)(L*N)dL)
      x = fragment position
      V = view vector
      Ln = nth light vector
      L0(x,V) <- final color
      Le(x,V) <- light emitted
      SUM_n(
        fr(x,Ln,V) <- BRDF(bidirectional reflectance distribution function)
        Li(x,Ln) <- incoming light
        (Ln*N) <- dot(light, normal)
      )
      BRDF = kd*f_diffuse + ks*f_specular
        kd + ks = 1
        ks = fresnel, kd = 1 - ks
      f_schlick = f0 + (1-f0)(1-(v.h))^5
        f0 = base reflectivity
        v = view vector
        h = half-way vector
      diffuse: lambertian vs oren-nayar
        f_lambert = coor/pi * dot(Ln, 
      cook-torrence
        dgf/4(V*N)(L*N)
      D = normal distribution function
        GGX/TROWBRIDGE-pei, has pi, a = roughness square
      G = geometry shadowin
        smith-ggx
          geometry obstruction, geometry shadowing
        Gsmith = G1 ...
        k = a/2 // unreal, etc
       
  rule of thumb: no negative dot products, no division by 0
    max(dot(x,y), 0.0)
    42/max(v, 0.00001);

    frag shader inputs:
      vec3 fragPos,
      vec3 normal,
      vec3 cameraPos,
      vec3 lightDir,
      vec3 lightPos,
      vec3 lightColor,
      vec3 albedo,
      vec3 emissivity,
      float roughness,
      vec3 baseReflectance,
      float metallic, // only reflects specular, not diffuse ?

    N = normalize(normal)
    V = normalize(cameraPos - fragPos)
    // for directional lights
    L = normalize(lightDir)
    // for point lights
    L = normalize(lightPos - fragmentPos)
    H = normalize(V + L)

  fn D(alpha, N, H) {
    numerator = pow(alpha, 2.0)
    NdotH = max(dot(N,H), 0.0)
    denominator = PI * pow(pow(NdotH, 2.0) * (pow(alpha, 2.0) - 1.0) + 1.0, 2.0)
    denominator = max(denominator, 0.000001)

    return numerator/denominator;
  }
  fn G1(alpha, N, X) {
    numerator = max(dot(N,X), 0.0)

    k = alpha/2.0
    denominator = max(dot(N,X),0.0) * (1.0 - k) + k
    denominator = max(denominator, 0.000001)

    return numerator / denominator;
  }
  fn G(alpha, N, V, L) {
    return G1(alpha, N, V) * G1(alpha, N, L)
  }
  fn F(F0, V, H) {
    return F0 + (vec3(1.0) - F0) * pow(1 - max(dot(V, H), 0.0), 5.0)
  }
  // for one light source
  fn PBR() {
    Ks = F(F0, V, H) // lazy F0 is albedo
    Kd = (vec3(1.0) - Ks) * (1.0 - metallic) // note: need image based lighting for good looking metals

    lambert = albedo / PI

    cookTorNum = D(alpha, N, H) * G(alpha, N, V, L) * F(F0, V, H)
    cookTorDen = 4.0 * max(dot(V,N), 0.0) * max(dot(L,N), 0.0)
    cookTorDen = max(cookTorDen, 0.000001)
    cookTor = cookTorNum / cookTorDen

    BRDF = Kd * lambert + cookTorrance
    outgoingLight = emissivity + BRDF * lightColor * max(dot(L, N), 0.0);

    return outgoingLight;
  }
  
  frag shader:
    import all variables

    define normal distribution function
    define geometry shadowing function
    define fresnel effect function

    calculate Ks, Kd
    calculate diffuse component of the BRDF
    calculate specular component of the BRDF
    combine them into the BRDF

    insert the BRDF and other vars into rendering equation
    output result

Games with non-traditional rendering that look good:
  (that might not be hard to implement)
  popsok, snkrx, pistol whip, super hot, sayonara, destropolis

World building inspiration vids:
  https://www.youtube.com/watch?v=ELiqWceCk0Q tiny poly world
  https://www.youtube.com/watch?v=kQ0x0R_yHrs fantasy scape, walking
  https://www.youtube.com/watch?v=iNYUG1-A7t8 bob ross 5min, 50min, 5days

  space/nebula:
    https://www.youtube.com/watch?v=gSxxnR3aLjc cggeek space
    https://www.youtube.com/watch?v=c4Wec0HtFLE The Great Attractor - 4k 3D Nebula Made In Blender
    https://www.youtube.com/watch?v=3g8XxmqSP90
    noise textures?
      https://simon-thommes.com/free-stuff

Depth and stencil:
  https://open.gl/depthstencils

Writing a real time PBR shader:
  https://learnopengl.com/PBR/Theory
  Real-Time Rendering book

Gamma correction / "Importance of Being Linear":
  https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-24-importance-being-linear

More on gamma:
  Unity on linear color space:
    https://docs.unity3d.com/Manual/LinearLighting.html
  https://learnopengl.com/Advanced-Lighting/Gamma-Correction
  monitors darken colors, so if u don't gamma correct u tend 
    to over brighten (for physical correctness)
  For sRGB: #000->#888->#fff is perceptually uniform-ish
    so doubling brightness is 
  In CSS, u want perceptual uniformity. *1.1 should brighten preceptually by 10%
  In Games, u want physical uniformity. *1.1 should behave like putting 10% more photons out

ACES tonemapping:
  ?
  https://github.com/ampas/aces-dev
  https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/
    float3 ACESFilm(float3 x)
    {
        float a = 2.51f;
        float b = 0.03f;
        float c = 2.43f;
        float d = 0.59f;
        float e = 0.14f;
        return saturate((x*(a*x+b))/(x*(c*x+d)+e));
    }
  https://www.youtube.com/watch?v=DX5tQix9NbY <- useless acadamy vid
  https://chrisbrejon.com/cg-cinematography/chapter-1-5-academy-color-encoding-system-aces/

Rendering a sphere on a quad (and fitting it into a raster scene):
  https://bgolus.medium.com/rendering-a-sphere-on-a-quad-13c92025570c

Outlines / edge detection:
  https://twitter.com/ianmaclarty/status/1499494878908403712
  its depth (distance from camera), surface normal and colour
  colors stored in B channel as index into pallette

  https://twitter.com/GravitonPunch/status/1528921394251038720
    "For the most part, the light edges are convex and the dark edges are concave, based on the 
    normals of nearby pixels. However, I also look for large differences in depth and unlit 
    color and add additional dark edges."

  https://blender.community/c/rightclickselect/J9bbbc/?sorting=hot
    https://en.wikipedia.org/wiki/Prewitt_operator

  https://alexanderameye.github.io/notes/rendering-outlines/
  https://roystan.net/articles/outline-shader.html
    "Instead, we will modulate depthThreshold by the surface's normal. 
    Surfaces that are at a greater angle from the camera with have a larger 
    threshold, while surface that are flatter, or more planar to the camera 
    will have a lower threshold.
    To implement this we will need the normal of each surface, and the direction 
    from the camera to the surface (the view direction). We already have the 
    normal, but we don't have access to the view direction."

  Edge highlighting a la Supreme Commander:
    https://forum.unity.com/threads/can-i-achieve-this-edge-highlight-effect.831148/#post-6209342

  Related to SSAO?
    Eevee can do it by rendering the objects into a full screen depth texture 
    and or full screen normals texture, and then doing an inverse SSAO pass 
    with that. SSAO which usually requires 3+ more passes (initial gather 
    pass, blurring passes, maybe temporal reprojection) depending on how its 
    done to get something that's not needlessly expensive or noisy.

  https://docs.unity3d.com/550/Documentation/Manual/script-EdgeDetectEffectNormals.html
  https://forum.unity.com/threads/voxel-edge-smooth-effect.858589/#post-7014547
    > This is a screen space effect using normal texture and depth. If you 
    look at the Blender code, then they have two concepts "curvature" 
    (darkens the far edges and brightens the near edges) and "cavity" 
    (this is an AO effect that makes the edges smoother).
    https://github.com/malyawka/URP-ScreenSpaceCavity

  @bgolus on proper outline depth usage:
    https://twitter.com/bgolus/status/1532830971573129216
    "The fix is this don't compare the depth in view space, use the 
    depth along the normal. To do this you need to calculate the world 
    position of each sampled depth position, and then you get the "depth" 
    by subtracting the center depth from the offset depths, and dot with 
    the normal."
    
  https://atyuwen.github.io/posts/normal-reconstruction/

Linearize depth:
  https://learnopengl.com/Advanced-OpenGL/Depth-testing
  https://learnopengl.com/Advanced-Lighting/Shadows/Shadow-Mapping
  float LinearizeDepth(float depth)
  {
      float z = depth * 2.0 - 1.0; // Back to NDC 
      return (2.0 * near_plane * far_plane) / (far_plane + near_plane - z * (far_plane - near_plane));
  }


Particles and techniques:
  http://www.opengl-tutorial.org/intermediate-tutorials/billboards-particles/particles-instancing/
  smooth particles: "test if the currently-drawn fragment is near the Z-Buffer. 
    If so, the fragment is faded out."

Bevy used destiny for inspiration on pipeline architecture:
  https://advances.realtimerendering.com/destiny/gdc_2015/Tatarchuk_GDC_2015__Destiny_Renderer_web.pdf
  https://www.youtube.com/watch?v=0nTDFLMLX9k

Presentation on WebGPU by Austin Eng:
  https://docs.google.com/presentation/d/1URnqb1Vuf2jPieHnt_eqXsPV_Es9Oog00_8LKZUdo6g/edit#slide=id.g482a63b4f5_0_494
  https://docs.google.com/presentation/d/1Z_3-3V6FRsF8OJNeH7yc6UKtgXy90Ggff07V9Z6uo6U/edit#slide=id.g644e7765b4b81e56_540
  
UPenn class on GPU programming:
  https://cis565-fall-2021.github.io
    https://github.com/chetan-parthiban/Project5-WebGL-Forward-Plus-and-Clustered-Deferred

What are mesh shaders?
  https://www.reddit.com/r/gamedev/comments/fn1byx/whats_is_mesh_shading/
  Mesh shaders are basically compute shaders that can output triangles (<256) by themselves.
  https://www.youtube.com/watch?v=CFXKTXtil34
  https://blog.siggraph.org/2021/04/mesh-shaders-release-the-intrinsic-power-of-a-gpu.html/
  WebGPU support (post v1):
    https://github.com/gpuweb/gpuweb/issues/3015
  Likely emulatable with compute shaders

Some tips on improving Unity's default look:
  https://www.youtube.com/watch?v=f6zUot73-gg

Destiny's render engine:
  https://www.youtube.com/watch?v=0nTDFLMLX9k
  Per object, they seperate:
    game object state (simulation stuff)
    visibility state
    render state
  once per frame they run visibility then extract render state based
    only on what is visible
  views: player, shadow, overhead, etc
    == render job chain unit
    visibility, extract, prepare, submit
  we should really have GPU counters for perf measurement

VFX:
  How (not) to create textures for VFX: https://www.youtube.com/watch?v=KaNDezgsg4M
  mentions: Jason keyser, alex redfish, 1mafx, ducvu fx
  texture types: 
    sprite, 
    sprite sheet, 
    flip book, 
    volumetric 
  texture uses: 
    color, 
    masks (incl erosion, disolve), 
    LUT (colorize grayscale), 
    vector maps (distort UVs)
  texture creation:
    handmade (paint: 🏃‍♀️, photo bash: ⚡️)
    procedural (substance designer: 🏃‍♀️, simple tools: ⚡️),
    simulate (blender: 🐌, houdini: 🐌, embergen: 🏃‍♀️)
    use existing: ⚡️⚡️ (texture packs, brushes)
  https://mebiusbox.github.io/contents/EffectTextureMaker/ (very cool!)
  https://www.escapemotions.com/products/flamepainter/try/
  talk resources:
    https://simonschreibt.de

Doom 3 glow:
  https://simonschreibt.de/gat/doom-3-volumetric-glow
  http://yzergame.com/doomGlare.html
    https://www.reddit.com/r/gamedev/comments/1wukvd/doom_3_glare_effect_in_c/
    https://gist.github.com/TiliSleepStealer/e6773e7f9b4a873cf1bffe4f9e11478f

"5 types of dithering":
  https://twitter.com/KilledByAPixel/status/1546532593948057601
  Diffusion - Floyd-Steinberg
  Ordered - Bayer Matrix
  Halftone - Newsprint
  LCD - RGB Screen Pixels
  Pixel - Squares
  These combine with extra effects like chroma shift, glitch, invert, low res, and paper color.

"Next Generation Geometry" / shader culling:
  https://timur.hu/blog/2022/what-is-ngg
  mesh shaders are dope
  shader culling is a new cool thing we can now do

Puffy stylized clouds:
  https://nokdef.com/Articles/Stylized-Puffy-Clouds-Shader/

Drawing lines is hard:
  https://mattdesl.svbtle.com/drawing-lines-is-hard

Anti-aliasing:
  rendering line primatives on edges:
    https://people.csail.mit.edu/ericchan/articles/prefilter/

"Efficently rendering glTF models, A WebGPU Case Study":
  https://toji.github.io/webgpu-gltf-case-study/
  Render pipelines are also fairly expensive to create, and can cause hitches if you create them while rendering.
  Calls to setPipeline() should be treated as expensive, because they generally are! 

"Deferred texturing"
  https://therealmjp.github.io/posts/bindless-texturing-for-deferred-rendering-and-decals/
  instead of writing out a G-Buffer containing all of the material parameters required for shading, you instead write out your interpolated UV’s as well as a material ID. 
    The main benefit is that you can ensure that your textures are only sampled for visible pixels
  For normal mapping you need your full tangent frame, which at minimum requires a quaternion
  For mipmaps and anisotropic filtering we also need the screen-space derivatives of our UV’s
  https://www.reedbeta.com/blog/deferred-texturing/

"GPU-Driven Rendering Pipelines", Ubisoft:
  http://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf

"GPU-driven rendering":
  https://stackoverflow.com/questions/59686151/what-is-gpu-driven-rendering

"Adventures in Text Rendering: Kerning and Glyph Atlases":
  https://www.warp.dev/blog/adventures-text-rendering-kerning-glyph-atlases

"Stuff I did over the years":
  https://twitter.com/Mirko_Salm/status/1553724647664926721

"Sokpop: I built a 3D engine in 2D":
  https://www.youtube.com/watch?v=chtRPC1ISyA
  example project: https://sokpop.itch.io/sokpop-fake-3d-demo

Decals:
  https://tuket.github.io/posts/2022-08-05-explosion-decals/

Falconeer clouds:
  https://youtu.be/5d8tx6K6hkk?t=6896
  spheres, noise texture, fresnel effect to hide outside edges
  tri-planar mapping to ea sphere
    e.g. tri-planar: https://gamedevelopment.tutsplus.com/articles/use-tri-planar-texture-mapping-for-better-terrain--gamedev-13821
  clouds are lit by using the skybox gradient (not lit on sphere)
    "if i were to turn off the distortion, they would disappear into the sky"
  sorting and clipping isn't an issue since clouds are so similar w/ tri-planar mapping
  still does alpha sorting?

Falconeer floodlights:
  triangular prism,
  addative,
  fade out w/ depth blend so u don't get ugly intersections

Reternal particle system:
  This is a great talk about the custom particle system used in Reternal for everything incl tentacles
  https://www.youtube.com/watch?v=qbkb8ap7vts
  This could be a great starting point for our own particle system, and I think it fits nicely with the Cy workflow

  "Semi-Lagrangian grid-based fluid sim"
    centered around the player, running at all time
      simulates air flow
    particles can simple from it
      and affect it?