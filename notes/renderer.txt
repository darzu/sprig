WebGL
  https://twgljs.org // tiny layer over webgl
  https://twgljs.org/examples/twgl-cube.html
  https://twgljs.org/examples/webgl-cube.html
  https://github.com/pmndrs/react-three-fiber
  https://enable3d.io (three.js, ammo.js, capacitor.js, phaser, )
  https://github.com/tamani-coding/enable3d-physics-examples

Three.js:
  https://threejs.org
  https://www.npmtrends.com/babylonjs-vs-three


WebGL learning:
  https://games.greggman.com/game/webgl-3d-cameras/
  https://webglfundamentals.org/webgl/lessons/webgl-3d-camera.html
  https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API/WebGL_best_practices
  https://www.tutorialspoint.com/webgl/webgl_interactive_cube.htm

Babylon:
  https://www.babylonjs.com
  https://playground.babylonjs.com
  https://nme.babylonjs.com // material editor (flow based)
  https://github.com/BabylonJS/SummerFestival
  https://doc.babylonjs.com/guidedLearning/createAGame
  https://babylonjs.medium.com/from-unity-to-babylon-js-how-is-the-journey-c71f79482aa3
  https://news.ycombinator.com/from?site=babylonjs.com

Three.js tutorial:
  https://www.youtube.com/watch?v=cp-H_6VODko

HAXE:
  http://babylonhx.com
  https://haxe.org/use-cases/games/
    Northgard, Dead Cells, "Papers, Please", Rymdkapsel
  https://github.com/armory3d

  Heaps.io:
    https://heaps.io

Multi-platform:
  https://github.com/expo/expo/tree/master/packages/expo-gl#expo-gl

PlayCanvas:
  https://playcanvas.com
  praise: https://news.ycombinator.com/item?id=27050731
  https://news.ycombinator.com/from?site=playcanvas.com
  https://blog.playcanvas.com/a-multiplayer-3rd-person-shooter-in-html5/

Unity Tiny:
  https://unity.com/solutions/instant-games
  https://forum.unity.com/threads/project-tiny-0-32-preview-is-available-ui-new-skinned-mesh-renderer-blendshape-sample.1045204/
  https://tiny.vision/demos/Tiny3D/Wasm/Tiny3D.html (needs Chrome or Safari Preview)
  https://github.com/Unity-Technologies/ProjectTinySamples/tree/master/Tiny3D
  (OLD!) https://docs.unity3d.com/Packages/com.unity.tiny@0.13/manual/scripting-systems.html
  (OLD!) https://docs.unity3d.com/Packages/com.unity.tiny@0.13/manual/intro-for-unity-developers.html

Pict3D:
  https://docs.racket-lang.org/pict3d/index.html

Regl:
  https://github.com/regl-project/regl

e.g.
  https://github.com/jacklaplante/bowdown
  https://github.com/onegeek/webglu

Sketch fab:
  https://sketchfab.com

Defold:
  https://defold.com
    "I recall our unity engineer quite liked Defold, which had an 
    integrated builder tool, but for some reason the lead dev didn’t 
    want to go with it." - https://news.ycombinator.com/item?id=24018097
  https://github.com/defold/defold
  Owned by King (Candy Crush, etc)

Google Docs switching to canvas:
  https://workspaceupdates.googleblog.com/2021/05/Google-Docs-Canvas-Based-Rendering-Update.html

Flutter does all UI via canvas:
  https://gallery.flutter.dev/#/

Construct:
  https://www.construct.net/en/make-games/showcase

WebGPU
    https://github.com/gpuweb/gpuweb
    status:
        https://github.com/gpuweb/gpuweb/wiki/Implementation-Status
    spec:
        https://gpuweb.github.io/gpuweb/
    examples:
      https://www.willusher.io/projects#WebGPU%20Experiments

Construct blog:
  https://www.construct.net/en/blogs/ashleys-blog-2/brief-history-graphics-web-1517
  https://www.construct.net/en/blogs/ashleys-blog-2/webgl-webgpu-construct-1519

Safari WebGPU intro:
  https://webkit.org/blog/9528/webgpu-and-wsl-in-safari/

Mozilla WebGPU intro:
  https://hacks.mozilla.org/2020/04/experimental-webgpu-in-firefox/

Chrome WebGPU intro:
  https://www.youtube.com/watch?v=K2JzIUIHIhc
  
TO DECIDE:
    Build our own renderer?

Understand URP:
    https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@12.0/manual/universalrp-builtin-feature-comparison.html'

GLTF viewers:
    https://sandbox.babylonjs.com/
    https://gltf-viewer.donmccurdy.com/
    https://gltf.insimo.com/
    https://github.khronos.org/glTF-Sample-Viewer-Release/
        https://github.com/KhronosGroup/glTF-Sample-Viewer

GLTF loader:
    https://github.com/mrdoob/three.js/blob/a98b9bf/examples/js/loaders/GLTFLoader.js

GLTF compression:
    https://google.github.io/draco/
        has JS encode / decode libraries

WebGPU on Chrome:
    https://developers.google.com/web/updates/2019/08/get-started-with-gpu-compute-on-the-web

WebGPU tutorials:
    https://alain.xyz/blog/raw-webgpu
        matrix library: https://github.com/toji/gl-matrix
    https://www.willusher.io/graphics/2020/06/15/0-to-gltf-triangle

WebGPU samples:
    https://github.com/mikbry/awesome-webgpu
    http://austin-eng.com/webgpu-samples/samples/computeBoids

Vulkan vs OpenGL:
    https://gamedev.stackexchange.com/questions/96014/what-is-vulkan-and-how-does-it-differ-from-opengl

WebGPU uses Web Shading Language (WSL)

webgpu-rs:
    https://github.com/gfx-rs/wgpu-rs
    Why WebGPU for native is good:
      http://kvark.github.io/web/gpu/native/2020/05/03/point-of-webgpu-native.html
    https://dawn.googlesource.com/dawn (webgpu on native by Google)

MIT computer graphics lectures:
    https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-837-computer-graphics-fall-2012/lecture-notes/
    lectures
      https://www.youtube.com/watch?v=-LqUu61oRdk&list=PLQ3UicqQtfNuBjzJ-KEWmG1yjiRMXYKhh
      https://www.youtube.com/watch?v=t7g2oaNs-c8&list=PLQ3UicqQtfNuKZjdA3fY1_X9gXn13JLlW

Game Engine Architecture book:
    https://www.gameenginebook.com/

RTX / ray tracing:
  minecraft RTX: https://alain.xyz/blog/frame-analysis-minecraftrtx
  on m1 https://www.willusher.io/graphics/2020/12/20/rt-dive-m1
    ~7-9 million rays / sec
  beyond ray-tracing http://sci.utah.edu/~will/papers/rtx-points-hpg19.pdf
  on GPU via Cuda: https://developer.nvidia.com/blog/accelerated-ray-tracing-cuda/
  Ray Tracing Gems II:
    http://www.realtimerendering.com/raytracinggems/rtg2/index.html

pbr / physically based rendering:
  https://pbrt.org
  https://www.pbr-book.org/3ed-2018/contents

GPU API concepts compared:
  https://alain.xyz/blog/comparison-of-modern-graphics-apis

Defered rendering:
  https://gamedev.stackexchange.com/questions/74/what-is-deferred-rendering
    Forward: O(geometry * lights)
    Defered: O(geometry + lights)
  Works poorly for transparency (most engines use Forward)
    see: depth peeling
  Uses large amounts of VRAM and frame buffer bandwidth
  stencil-based geometry vs "tiled/froxel compute-based shading"
  Unity: https://docs.unity3d.com/2021.2/Documentation/Manual/RenderTech-DeferredShading.html
  https://www.reddit.com/r/gamedev/comments/8klygv/is_deferred_shading_still_considered_state_of_the/
    "It looks like the industry is going towards a forward+/hybrid approach"
      0. Render Depth Prepass (optional, could prepare a thin gbuffer on this pass)
      1. Use depth buffer (or not) to bin lights into screenspace tiles, save this as a light buffer
      2. Render the geometry with the full shader, and get the light information from the screenspace coordinate, wich lets you see the lights on that tile
      3. Postprocess.
    "Forward+ splits the screen into a 2d grid and a process (compute or other shader) figures out what lights affect that tile. Forward++ takes this a step 
    further and instead of a 2d grid splitting the screen, its a 3d grid splitting space with perspective."
  used by Horizon Zero Dawn

Shadows:
  Shadow volumes? (aka stencil shadows)
    https://en.wikipedia.org/wiki/Shadow_volume
  Shadow mapping
    https://www.youtube.com/watch?v=o6zDfDkOFIc
    use the normal view frustum to compute the bounding box that should be used for the shadow map
      compute view frustum with different far-clipping plane to adjust shadow distance

Lighting:
  pixel vs vertex
    could subdivide large triangles so vertex lighting is more accurate; no storage cost
    vertex + tessellation for particles (e.g. smoke):
      http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.187&rep=rep1&type=pdf
  Spherical Harmonics lighting
    https://computergraphics.stackexchange.com/questions/4164/what-are-spherical-harmonics-light-probes
    https://mynameismjp.wordpress.com/2016/10/09/sg-series-part-1-a-brief-and-incomplete-history-of-baked-lighting-representations/
  deferred vs forward
  https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/shading-normals
  WebGPU "clustered forward shading"
    https://toji.github.io/webgpu-clustered-shading/
    https://github.com/toji/webgpu-clustered-shading
      "I don't think WGSL has atomic methods yet so I can't effectively do the light list compacting 
      the way I want (or at least I don't know the workaround)."

TO LEARN:
  Fourier transform
    then Spherical Harmonics

GPU perf for artists:
  http://www.fragmentbuffer.com/gpu-performance-for-game-artists/
    https://news.ycombinator.com/item?id=14726355
    https://news.ycombinator.com/item?id=21978146

Frame analysis:
  compilation:
    http://www.adriancourreges.com/blog/
  RenderDoc:
    https://renderdoc.org
    https://spector.babylonjs.com (for WebGL)
  https://zhangdoa.com/posts/rendering-analysis-cyberpunk-2077
  https://alain.xyz/blog/frame-analysis-overwatch
  https://alain.xyz/blog/frame-analysis-mk11
  https://alain.xyz/blog/frame-analysis-minecraftrtx
  https://aschrein.github.io/2019/08/11/metro_breakdown.html
  https://aschrein.github.io/2019/08/01/re2_breakdown.html
  http://www.adriancourreges.com/blog/2016/09/09/doom-2016-graphics-study/
    https://news.ycombinator.com/item?id=12461896
    http://advances.realtimerendering.com/s2016/Siggraph2016_idTech6.pdf
  http://www.adriancourreges.com/blog/2017/12/15/mgs-v-graphics-study/
  http://www.adriancourreges.com/blog/2015/11/02/gta-v-graphics-study/
    https://news.ycombinator.com/item?id=10492876
  http://www.adriancourreges.com/blog/2015/06/23/supreme-commander-graphics-study/
    https://news.ycombinator.com/item?id=9770020
  http://www.adriancourreges.com/blog/2015/03/10/deus-ex-human-revolution-graphics-study/
    https://news.ycombinator.com/item?id=9565891
  https://mamoniem.com/behind-the-pretty-frames-elden-ring/

BLOGS TO SCAN:
  https://www.ea.com/frostbite/news
  http://advances.realtimerendering.com/s2015/index.html (siggraph conference)
  https://simonschreibt.de/game-art-tricks/
  https://www.willusher.io
    lots of graphics projects: https://www.willusher.io/projects
    teapot rendering challenge https://graphics.cs.utah.edu/trc/
  https://www.cs.washington.edu/research/graphics
    https://grail.cs.washington.edu/research/
    http://courses.cs.washington.edu/courses/cse457/ Computer Graphics
    http://courses.cs.washington.edu/courses/cse458/ Computer Animation
    http://courses.cs.washington.edu/courses/csep557/ Trends in Computer Graphics
    http://courses.cs.washington.edu/courses/cse557/ Computer Graphics
  http://madebyevan.com
  https://mamoniem.com

coloring models:
  https://www.willusher.io/webgl-ewa-splatter/#Dinosaur

TO READ:
  https://forum.beyond3d.com/threads/gpu-driven-rendering-siggraph-2015-follow-up.57240/
    "about GPU-driven rendering pipelines"

GDC:
  Horizon Zero Dawn:
    Vegitation https://www.youtube.com/watch?v=wavnKZNSYqU
    game design https://www.youtube.com/watch?v=TawhcWao9ls
    procedural gen https://www.youtube.com/watch?v=ToCozpl1sYY

Rust -> WebGPU examples:
  https://github.com/gfx-rs/wgpu-rs/tree/master/examples
  https://wgpu.rs

"Mixed resolution rendering":
  https://www.gdcvault.com/play/1022982/Mixed-Resolution-Rendering-in-Skylanders
    for clouds based on sprites
    downsample depth -> raster -> "bilateral" upsample 

LOD / automatic mesh simplification:
  quad simplification: https://www.youtube.com/watch?v=vBJcdClynFE

Rendering pipeline:
  https://www.khronos.org/opengl/wiki/Rendering_Pipeline_Overview

hierarchical /  models:
  https://sites.google.com/site/csc8820/educational/how-to-animate-hierarchical-models
  https://canvas.dartmouth.edu/courses/16840/assignments/82764

how many triangles?
  "overwatch ingame characters has 60K tris"

"bundles"?
  https://computergraphics.stackexchange.com/questions/4066/whats-the-main-difference-of-pipeline-process-between-vulkan-and-dx12

Why r draw calls expensive?
  "because if you send too little to the GPU, you're CPU bound and the GPU idles"
  https://stackoverflow.com/questions/4853856/why-are-draw-calls-expensive
  https://www.nvidia.com/docs/IO/8228/BatchBatchBatch.pdf

Data per triangle:
  "instanced vertex attributes"
  gl_PrimitiveID
  "texture buffer object (TBO)" for high primitive count
    texelFetch

Graphics storage options:
  Vertex buffer
  Vertex buffer (instance step)
  Uniform buffer
    Small, performant
  SSBOS
    large, slow, writable

Unreal 5 analysis:
  https://www.elopezr.com/a-macro-view-of-nanite/

Camera transformations:
  https://www.3dgep.com/understanding-the-view-matrix/#Transformations
  http://www.codinglabs.net/article_world_view_projection_matrix.aspx
  https://gamedev.stackexchange.com/questions/178643/the-view-matrix-finally-explained
  https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix
  http://web.cse.ohio-state.edu/~wang.3602/courses/cse5542-2013-spring/6-Transformation_II.pdf
  https://learnopengl.com/Getting-started/Coordinate-Systems

hierarchical modeling:
  https://www.youtube.com/watch?v=JdhpViedm0g&list=PLQ3UicqQtfNuBjzJ-KEWmG1yjiRMXYKhh&index=5

Low-poly planet map effect:
  https://www.youtube.com/watch?v=i5zwDoYXH5c

WebGPU vs WebGL:
  https://www.babylonjs.com/demos/webgpu/forestwebgl
  https://www.babylonjs.com/demos/webgpu/forestwebgpu

Alpha tested vs transparency:
  https://forum.unity.com/threads/difference-between-alphatest-and-transparent-renderqueue.458750/
  alpha tested renders front-to-back (occluded pixels r ignored)
  transparency renders back-to-front

transparency:
  http://learnwebgl.brown37.net/11_advanced_rendering/alpha_blending.html
    in the standard technique, u have to order transparent objects back to front and
      render them in a seperate pass
    b/c alpha blending is not commutitive in real life; 
      a 99% opaque object in front will dominate the result color
    unless: 
      only one transparent object; 
      transparent objects never overlap in the camera;
    use insertion sort b/c less work when nothing changes frame to frame

"order independent transparency":
  https://interplayoflight.wordpress.com/2022/06/25/order-independent-transparency-part-1/
  https://interplayoflight.wordpress.com/2022/07/02/order-independent-transparency-part-2/

Texture compression:
  BC7, BC4, BCn
  https://www.reedbeta.com/blog/understanding-bcn-texture-compression-formats/

Foveated rendering:
  https://en.wikipedia.org/wiki/Foveated_rendering

GPU hardware:
  Rasterization is still done in fixed hardware, not programmable
    https://en.wikipedia.org/wiki/Rasterisation
    (Intel tried with Larrabee, aborted)
    (Nvidia tried: https://highperformancegraphics.net/previous/www_2011/media/Papers/HPG2011_Papers_Laine.pdf, 
      https://dl.acm.org/doi/10.1145/2018323.2018337, http://code.google.com/p/cudaraster/)
      Issues: interpolation, anti-aliasing, power consumption
    Desire for software rasterization:
      shader perf boost?
      ROP (render output unit): a-buffering, order-independent transparency
      Stochastic rasterization
      Non-linear rasterization
  Vertex and fragment shaders
  Compute vs Fragment for post-processing:
    https://computergraphics.stackexchange.com/questions/54/when-is-a-compute-shader-more-efficient-than-a-pixel-shader-for-image-filtering
  ROP:
    https://en.wikipedia.org/wiki/Render_output_unit
    handles anti-aliasing (e.g. MSAA)

Z-order curve:
  https://en.wikipedia.org/wiki/Z-order_curve#Applications
  Zig-zag textures for more efficient cache locality

Albedo vs Diffuse:
  https://computergraphics.stackexchange.com/questions/350/albedo-vs-diffuse

Virtual texturing:
  http://holger.dammertz.org/stuff/notes_VirtualTexturing.html
  https://computergraphics.stackexchange.com/questions/1768/how-can-virtual-texturing-actually-be-efficient

Don't have T junctions in meshes:
  https://computergraphics.stackexchange.com/questions/1461/why-do-t-junctions-in-meshes-result-in-cracks

Frustum culling:
  https://www.iquilezles.org/www/articles/frustumcorrect/frustumcorrect.htm

ALTERNATE APPROACH:
  It could be worth considering going for a low-res ray tracing. E.g. maybe just 256x256 pixels, but 
  ray tracing them all.
  This would let us drastically simplify the renderer and do the cool RTX techniques
  Using SDF tech, we could get some very cool meshes and mesh building

SDF for text:
  https://steamcdn-a.akamaihd.net/apps/valve/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf
  https://www.youtube.com/watch?v=1b5hIMqz_wM
    Material Maker
  text is hard?
    https://gankra.github.io/blah/text-hates-you/
  https://www.qt.io/blog/2011/07/15/text-rendering-in-the-qml-scene-graph
  https://computergraphics.stackexchange.com/questions/306/sharp-corners-with-signed-distance-fields-fonts

VFX explosions:
  https://www.youtube.com/watch?v=dDsb_9n-Gik

Flow maps and gradient maps for moving textures:
  https://www.youtube.com/watch?v=KfphtLRoUB0

Jump Flood Algorithm:
  Ben: https://bgolus.medium.com/the-quest-for-very-wide-outlines-ba82ed442cd9
  https://prideout.net/blog/distance_fields/
  https://blog.demofox.org/2016/02/29/fast-voronoi-diagrams-and-distance-dield-textures-on-the-gpu-with-the-jump-flooding-algorithm/
  https://shaderbits.com/blog/various-distance-field-generation-techniques
    https://www.shadertoy.com/view/4syGWK
  Freya: https://twitter.com/freyaholmer/status/1292536599230775296?lang=en
  https://www.youtube.com/watch?v=A0pxY9QsgJE

  can u use JFA to blur stars instead of gauss blur? easier to make arbitrarily big,
    maybe easier to modulate based on distance to camera

  "Triangle Voronoi Borders":
    https://www.shadertoy.com/view/ss3fW4

  Related to voronoi?
    https://www.redblobgames.com/x/2022-voronoi-maps-tutorial/
    Delaunator: https://github.com/mapbox/delaunator (good license)
      guide: https://mapbox.github.io/delaunator/
    Delaunator seems good for CPU and gives a mesh; unclear about GPU perf.

  
Outlines
  https://bgolus.medium.com/the-quest-for-very-wide-outlines-ba82ed442cd9
  overwatch and control use "sobol" for outlines?
    https://www.youtube.com/watch?v=hxPQ2F96F9E
    looks like a sampling technique:
      https://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction/Sobol_Sampler

Mesh simplification:
  https://www.simplygon.com
  techniques: billboard cloud, flip book

Noise is good in game art?:
  makes things less flat
  https://simblob.blogspot.com/2009/06/noise-in-game-art.html

Factory Station's rendering:
  https://twitter.com/GravitonPunch/status/1453072441316675584
  https://twitter.com/alexanderameye/status/1375463146446585857

Rendering problem cheat sheet:
  https://techartaid.com/cheatsheet/

To improve look and feel, I should write a .gltf exporter than load that in Unity 
  or blender and play with materials, lighting, shaders etc until it looks how I want.
  Then figure out what the delta is and how to impl it. Probably cut corners.

Waves:
  https://playground.babylonjs.com/?webgpu#YX6IB8#55
  https://github.com/gasgiant/FFT-Ocean

WebGPU breakout sample:
  https://github.com/toji/spookyball

Tomas Sala doesn't believe in textures either:
  https://youtu.be/5d8tx6K6hkk?t=3050
  no baking lights
  no diffuse maps, no normal maps,
  world-space fog gradients use extensively
  world-space aware shaders in general
  day-night cycle:
    one slider, indexes ~8 gradients
  having no textures means he has tons of memory:
    his audio is uncompressed
    his whole world is loaded in at once
    consistent 60fps without much effort
  doesn't do any special culling, just frustum culling built into Unity
  regarding low-poly:
    he doesn't like showing all the facetted edges,
    "smoothing groups" ?
    some vertices smooth together, some stay sharp with each other
  b/c models are "kitbashed", destruction just means flinging individual parts
  hmm seems he does bake some lights into vertex colors
  uses forward rendering for perf reasons
  for flood light: just use a box w/ additive color
  clouds: transparent spheres, z sorted ?
    ray marched, volumetric stuff is too expensive
  "i'm not smart enough to use that so I won't"
  "find the pillars you need and focus on those"
    In finding nemo, they had a small list of things that make ocean water work
    in the outdoor setting of falconeer, 
  "the smaller the prison the more exciting the break out"
    limitations breed creative solutions
  color correction: "that is so important"
  things need to feel consistent
    water color reflected in sky
    things need to feel solid, like they r one piece
  in unity the perf metrics: batches and tris
    <200 batches, <200,000 triangles for mobile
    <400 batches, <400,000 triangles for switch
      up to 600 batches, 600,000 triangles
    on PC, 1,000 batches, 1,000,000 triangles is safe
  ocean 
    is "occlusion shape" ? only rendered for camera
    ~50k triangles
    ocean shader done in Shader Forge
      omg, it's huge
    waves itself: vertex displacement, "gerstner wave" ?
      sine wave, absolute value, but soft tops?
      7 iterations, bigger and smaller, merged
      has mirror code in C# and shader for displacement
    has shadows on the water
    never got ambient occlusion to work on the water
    occasionally generate wave splaces
  for reflections:
    box blur is too expensive
    uses lots of jagged sine waves?
    planar reflection
  lip sync by just have emotional blabber blended to 
    mouth closed based on audio
  clouds:
    inspiration from dutch moody sky ship paintings
    "I'm not smart enough to understand how ray marched volumetric clouds really work"
    Ace Combat has blog on their clouds (too fancy?)
      https://www.skywardfm.com/post/the-form-and-function-of-clouds-and-weather-in-ace-combat-7
    Tomas vid on clouds: https://www.youtube.com/watch?v=4i3w9XmxYSo
      spheres
      fresnel + noise for soft outer transparency
      lighting underneath (eventually removed?)
        prevented b/c of "tri planar mapping"
        noise is world-space dependent
        light based on direction and sky gradient
      not particles, needed custom sorting
      sorting issues are less of problem b/c of tri planar mapping ?
        b/c clouds are so similar to their neighbors, u don't notice clipping/sorting
      only heavy b/c of alpha sorting
      sorting orthographicly on switch, "down axis"
    some high up clouds 
    global "light color" is shared between many many shaders
  skybox is a sphere
  combination of tricks makes it work:
    sphere clouds, sphere skybox, day/night lighting, water shader, 
    water reflections, water shadows, ground fog, 
    short render distance makes it feel more atmospheric
  ocean big displacements:
    giant ocean trench:
      displace waves based on worldspace z coordinate sine wave
      acquaduct bridge crossing the canyon
    sphere displacement, can even animate
    caves that are just clipping the ocean
    b/c displacement is world space, it's trivial to create another 
      plane using the same displacements and just compose them
      https://youtu.be/5d8tx6K6hkk?t=8103
  birds flap forward?
  for 120hz, only the bird and camera are frame rate independent
  uses scriptable objects to create missions
    would have loved to know unity editor scripting better
  made the game front to back: don't do that, b/c ur best work will be
    at the end. u want ur peak work and tools for the first bit
    people r not patient
  more re textureless: https://www.youtube.com/watch?v=Bu5mxNyR8uA
    the less u do with textures, the more u do with colors, interesting things start to happen
    about achieving an emotional effect with the least amount of detail
    spent a bunch of time on the pipeline (shaders etc) so that simple shapes go in and come out looking good
    for geometry, only two shaders: one glowy, one not
  another vid: https://www.youtube.com/watch?v=UhtkcRyGG6o
    just avoids adding things that he knows are hard and will take him out of the flow
    chasing certain bugs will just give writers block for a week
    uses "smoothing groups"
    UV coords, just make a gradient

Death's door uses fairly minimal textures:
  https://youtu.be/pcSmBGkbd-g?list=PLX2vGYjWbI0S44qONl7OmB5tpq1YaFN8F&t=3197

Font choices:
  Montserrat ? https://twitter.com/patriciogv/status/1526539963743059968

PBR:
  https://www.youtube.com/watch?v=RRE-F57fbXw
  common traits:
    energy conservation
    microfacet model
    fresnel effect
  rendering equation:
    L0(x,wo,l,t) = Le(x,wo,l,t) + INT_O(fr(x,wi,wo,l,t)Li(x,wi,l,t)(wi*n)dwi)
      x = frament position
      wo = outgoing light
      wi = negative incoming light
      l = wavelength
      t = time
    L0(x,V) = Le(x,V) + INT_O(fr(x,L,V)Li(x,L)(L*N)dL)
      x = fragment position
      V = view vector
      Ln = nth light vector
      L0(x,V) <- final color
      Le(x,V) <- light emitted
      SUM_n(
        fr(x,Ln,V) <- BRDF(bidirectional reflectance distribution function)
        Li(x,Ln) <- incoming light
        (Ln*N) <- dot(light, normal)
      )
      BRDF = kd*f_diffuse + ks*f_specular
        kd + ks = 1
        ks = fresnel, kd = 1 - ks
      f_schlick = f0 + (1-f0)(1-(v.h))^5
        f0 = base reflectivity
        v = view vector
        h = half-way vector
      diffuse: lambertian vs oren-nayar
        f_lambert = coor/pi * dot(Ln, 
      cook-torrence
        dgf/4(V*N)(L*N)
      D = normal distribution function
        GGX/TROWBRIDGE-pei, has pi, a = roughness square
      G = geometry shadowin
        smith-ggx
          geometry obstruction, geometry shadowing
        Gsmith = G1 ...
        k = a/2 // unreal, etc
       
  rule of thumb: no negative dot products, no division by 0
    max(dot(x,y), 0.0)
    42/max(v, 0.00001);

    frag shader inputs:
      vec3 fragPos,
      vec3 normal,
      vec3 cameraPos,w
      vec3 lightDir,
      vec3 lightPos,
      vec3 lightColor,
      vec3 albedo,
      vec3 emissivity,
      float roughness,
      vec3 baseReflectance,
      float metallic, // only reflects specular, not diffuse ?

    N = normalize(normal)
    V = normalize(cameraPos - fragPos)
    // for directional lights
    L = normalize(lightDir)
    // for point lights
    L = normalize(lightPos - fragmentPos)
    H = normalize(V + L)

  fn D(alpha, N, H) {
    numerator = pow(alpha, 2.0)
    NdotH = max(dot(N,H), 0.0)
    denominator = PI * pow(pow(NdotH, 2.0) * (pow(alpha, 2.0) - 1.0) + 1.0, 2.0)
    denominator = max(denominator, 0.000001)

    return numerator/denominator;
  }
  fn G1(alpha, N, X) {
    numerator = max(dot(N,X), 0.0)

    k = alpha/2.0
    denominator = max(dot(N,X),0.0) * (1.0 - k) + k
    denominator = max(denominator, 0.000001)

    return numerator / denominator;
  }
  fn G(alpha, N, V, L) {
    return G1(alpha, N, V) * G1(alpha, N, L)
  }
  fn F(F0, V, H) {
    return F0 + (vec3(1.0) - F0) * pow(1 - max(dot(V, H), 0.0), 5.0)
  }
  // for one light source
  fn PBR() {
    Ks = F(F0, V, H) // lazy F0 is albedo
    Kd = (vec3(1.0) - Ks) * (1.0 - metallic) // note: need image based lighting for good looking metals

    lambert = albedo / PI

    cookTorNum = D(alpha, N, H) * G(alpha, N, V, L) * F(F0, V, H)
    cookTorDen = 4.0 * max(dot(V,N), 0.0) * max(dot(L,N), 0.0)
    cookTorDen = max(cookTorDen, 0.000001)
    cookTor = cookTorNum / cookTorDen

    BRDF = Kd * lambert + cookTorrance
    outgoingLight = emissivity + BRDF * lightColor * max(dot(L, N), 0.0);

    return outgoingLight;
  }
  
  frag shader:
    import all variables

    define normal distribution function
    define geometry shadowing function
    define fresnel effect function

    calculate Ks, Kd
    calculate diffuse component of the BRDF
    calculate specular component of the BRDF
    combine them into the BRDF

    insert the BRDF and other vars into rendering equation
    output result

Games with non-traditional rendering that look good:
  (that might not be hard to implement)
  popsok, snkrx, pistol whip, super hot, sayonara, destropolis

World building inspiration vids:
  https://www.youtube.com/watch?v=ELiqWceCk0Q tiny poly world
  https://www.youtube.com/watch?v=kQ0x0R_yHrs fantasy scape, walking
  https://www.youtube.com/watch?v=iNYUG1-A7t8 bob ross 5min, 50min, 5days

  space/nebula:
    https://www.youtube.com/watch?v=gSxxnR3aLjc cggeek space
    https://www.youtube.com/watch?v=c4Wec0HtFLE The Great Attractor - 4k 3D Nebula Made In Blender
    https://www.youtube.com/watch?v=3g8XxmqSP90
    noise textures?
      https://simon-thommes.com/free-stuff

Depth and stencil:
  https://open.gl/depthstencils

Writing a real time PBR shader:
  https://learnopengl.com/PBR/Theory
  Real-Time Rendering book

Gamma correction / "Importance of Being Linear":
  https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-24-importance-being-linear

More on gamma:
  Unity on linear color space:
    https://docs.unity3d.com/Manual/LinearLighting.html
  https://learnopengl.com/Advanced-Lighting/Gamma-Correction
  monitors darken colors, so if u don't gamma correct u tend 
    to over brighten (for physical correctness)
  For sRGB: #000->#888->#fff is perceptually uniform-ish
    so doubling brightness is 
  In CSS, u want perceptual uniformity. *1.1 should brighten preceptually by 10%
  In Games, u want physical uniformity. *1.1 should behave like putting 10% more photons out

ACES tonemapping:
  ?
  https://github.com/ampas/aces-dev
  https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/
    float3 ACESFilm(float3 x)
    {
        float a = 2.51f;
        float b = 0.03f;
        float c = 2.43f;
        float d = 0.59f;
        float e = 0.14f;
        return saturate((x*(a*x+b))/(x*(c*x+d)+e));
    }
  https://www.youtube.com/watch?v=DX5tQix9NbY <- useless acadamy vid
  https://chrisbrejon.com/cg-cinematography/chapter-1-5-academy-color-encoding-system-aces/

Rendering a sphere on a quad (and fitting it into a raster scene):
  https://bgolus.medium.com/rendering-a-sphere-on-a-quad-13c92025570c

Outlines / edge detection:
  https://twitter.com/ianmaclarty/status/1499494878908403712
  its depth (distance from camera), surface normal and colour
  colors stored in B channel as index into pallette

  distant structures:
    https://twitter.com/ianmaclarty/status/1528884118703861761

  https://twitter.com/GravitonPunch/status/1528921394251038720
    "For the most part, the light edges are convex and the dark edges are concave, based on the 
    normals of nearby pixels. However, I also look for large differences in depth and unlit 
    color and add additional dark edges."

  screenspace curvature shader:
    blender: https://blender.community/c/rightclickselect/J9bbbc/?sorting=hot
      https://en.wikipedia.org/wiki/Prewitt_operator
    
    https://madebyevan.com/shaders/curvature/

    https://forum.unity.com/threads/released-screen-space-cavity-curvature-for-unity-inspired-by-blenders-cavity-effects.1261145/

    https://assetstore.unity.com/packages/vfx/shaders/fullscreen-camera-effects/screen-space-cavity-curvature-built-in-urp-hdrp-216995

  https://alexanderameye.github.io/notes/rendering-outlines/
  https://roystan.net/articles/outline-shader.html
    "Instead, we will modulate depthThreshold by the surface's normal. 
    Surfaces that are at a greater angle from the camera with have a larger 
    threshold, while surface that are flatter, or more planar to the camera 
    will have a lower threshold.
    To implement this we will need the normal of each surface, and the direction 
    from the camera to the surface (the view direction). We already have the 
    normal, but we don't have access to the view direction."

  Edge highlighting a la Supreme Commander:
    https://forum.unity.com/threads/can-i-achieve-this-edge-highlight-effect.831148/#post-6209342

  Related to SSAO?
    Eevee can do it by rendering the objects into a full screen depth texture 
    and or full screen normals texture, and then doing an inverse SSAO pass 
    with that. SSAO which usually requires 3+ more passes (initial gather 
    pass, blurring passes, maybe temporal reprojection) depending on how its 
    done to get something that's not needlessly expensive or noisy.

  https://docs.unity3d.com/550/Documentation/Manual/script-EdgeDetectEffectNormals.html
  https://forum.unity.com/threads/voxel-edge-smooth-effect.858589/#post-7014547
    > This is a screen space effect using normal texture and depth. If you 
    look at the Blender code, then they have two concepts "curvature" 
    (darkens the far edges and brightens the near edges) and "cavity" 
    (this is an AO effect that makes the edges smoother).
    https://github.com/malyawka/URP-ScreenSpaceCavity

  @bgolus on proper outline depth usage:
    https://twitter.com/bgolus/status/1532830971573129216
    "The fix is this don't compare the depth in view space, use the 
    depth along the normal. To do this you need to calculate the world 
    position of each sampled depth position, and then you get the "depth" 
    by subtracting the center depth from the offset depths, and dot with 
    the normal."
    
  https://atyuwen.github.io/posts/normal-reconstruction/

  https://twitter.com/csims314/status/1482123212188180490
  https://github.com/OmarShehata/webgl-outlines
  https://omar-shehata.medium.com/better-outline-rendering-using-surface-ids-with-webgl-e13cdab1fd94

  https://www.videopoetics.com/tutorials/pixel-perfect-outline-shaders-unity/

Linearize depth:
  https://learnopengl.com/Advanced-OpenGL/Depth-testing
  https://learnopengl.com/Advanced-Lighting/Shadows/Shadow-Mapping
  float LinearizeDepth(float depth)
  {
      float z = depth * 2.0 - 1.0; // Back to NDC 
      return (2.0 * near_plane * far_plane) / (far_plane + near_plane - z * (far_plane - near_plane));
  }

Particles and techniques:
  http://www.opengl-tutorial.org/intermediate-tutorials/billboards-particles/particles-instancing/
  smooth particles: "test if the currently-drawn fragment is near the Z-Buffer. 
    If so, the fragment is faded out."

Bevy used destiny for inspiration on pipeline architecture:
  https://advances.realtimerendering.com/destiny/gdc_2015/Tatarchuk_GDC_2015__Destiny_Renderer_web.pdf
  https://www.youtube.com/watch?v=0nTDFLMLX9k

Presentation on WebGPU by Austin Eng:
  https://docs.google.com/presentation/d/1URnqb1Vuf2jPieHnt_eqXsPV_Es9Oog00_8LKZUdo6g/edit#slide=id.g482a63b4f5_0_494
  https://docs.google.com/presentation/d/1Z_3-3V6FRsF8OJNeH7yc6UKtgXy90Ggff07V9Z6uo6U/edit#slide=id.g644e7765b4b81e56_540
  
UPenn class on GPU programming:
  https://cis565-fall-2021.github.io
    https://github.com/chetan-parthiban/Project5-WebGL-Forward-Plus-and-Clustered-Deferred

What are mesh shaders?
  https://www.reddit.com/r/gamedev/comments/fn1byx/whats_is_mesh_shading/
  Mesh shaders are basically compute shaders that can output triangles (<256) by themselves.
  https://www.youtube.com/watch?v=CFXKTXtil34
  https://blog.siggraph.org/2021/04/mesh-shaders-release-the-intrinsic-power-of-a-gpu.html/
  WebGPU support (post v1):
    https://github.com/gpuweb/gpuweb/issues/3015
  Likely emulatable with compute shaders

Some tips on improving Unity's default look:
  https://www.youtube.com/watch?v=f6zUot73-gg

Destiny's render engine:
  https://www.youtube.com/watch?v=0nTDFLMLX9k
  Per object, they seperate:
    game object state (simulation stuff)
    visibility state
    render state
  once per frame they run visibility then extract render state based
    only on what is visible
  views: player, shadow, overhead, etc
    == render job chain unit
    visibility, extract, prepare, submit
  we should really have GPU counters for perf measurement

VFX:
  How (not) to create textures for VFX: https://www.youtube.com/watch?v=KaNDezgsg4M
  mentions: Jason keyser, alex redfish, 1mafx, ducvu fx
  texture types: 
    sprite, 
    sprite sheet, 
    flip book, 
    volumetric 
  texture uses: 
    color, 
    masks (incl erosion, disolve), 
    LUT (colorize grayscale), 
    vector maps (distort UVs)
  texture creation:
    handmade (paint: 🏃‍♀️, photo bash: ⚡️)
    procedural (substance designer: 🏃‍♀️, simple tools: ⚡️),
    simulate (blender: 🐌, houdini: 🐌, embergen: 🏃‍♀️)
    use existing: ⚡️⚡️ (texture packs, brushes)
  https://mebiusbox.github.io/contents/EffectTextureMaker/ (very cool!)
  https://www.escapemotions.com/products/flamepainter/try/
  talk resources:
    https://simonschreibt.de

Doom 3 glow:
  https://simonschreibt.de/gat/doom-3-volumetric-glow
  http://yzergame.com/doomGlare.html
    https://www.reddit.com/r/gamedev/comments/1wukvd/doom_3_glare_effect_in_c/
    https://gist.github.com/TiliSleepStealer/e6773e7f9b4a873cf1bffe4f9e11478f

"5 types of dithering":
  https://twitter.com/KilledByAPixel/status/1546532593948057601
  Diffusion - Floyd-Steinberg
  Ordered - Bayer Matrix
  Halftone - Newsprint
  LCD - RGB Screen Pixels
  Pixel - Squares
  These combine with extra effects like chroma shift, glitch, invert, low res, and paper color.

dithering types:
  https://tannerhelland.com/2012/12/28/dithering-eleven-algorithms-source-code.html
  https://ieeexplore.ieee.org/document/3288


"Next Generation Geometry" / shader culling:
  https://timur.hu/blog/2022/what-is-ngg
  mesh shaders are dope
  shader culling is a new cool thing we can now do

Puffy stylized clouds:
  https://nokdef.com/Articles/Stylized-Puffy-Clouds-Shader/

Drawing lines is hard:
  https://mattdesl.svbtle.com/drawing-lines-is-hard

Anti-aliasing:
  rendering line primatives on edges:
    https://people.csail.mit.edu/ericchan/articles/prefilter/

"Efficently rendering glTF models, A WebGPU Case Study":
  https://toji.github.io/webgpu-gltf-case-study/
  Render pipelines are also fairly expensive to create, and can cause hitches if you create them while rendering.
  Calls to setPipeline() should be treated as expensive, because they generally are! 

"Deferred texturing"
  https://therealmjp.github.io/posts/bindless-texturing-for-deferred-rendering-and-decals/
  instead of writing out a G-Buffer containing all of the material parameters required for shading, you instead write out your interpolated UV’s as well as a material ID. 
    The main benefit is that you can ensure that your textures are only sampled for visible pixels
  For normal mapping you need your full tangent frame, which at minimum requires a quaternion
  For mipmaps and anisotropic filtering we also need the screen-space derivatives of our UV’s
  https://www.reedbeta.com/blog/deferred-texturing/

"GPU-Driven Rendering Pipelines", Ubisoft:
  http://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf

"GPU-driven rendering":
  https://stackoverflow.com/questions/59686151/what-is-gpu-driven-rendering

"Adventures in Text Rendering: Kerning and Glyph Atlases":
  https://www.warp.dev/blog/adventures-text-rendering-kerning-glyph-atlases

"Stuff I did over the years":
  https://twitter.com/Mirko_Salm/status/1553724647664926721

"Sokpop: I built a 3D engine in 2D":
  https://www.youtube.com/watch?v=chtRPC1ISyA
  example project: https://sokpop.itch.io/sokpop-fake-3d-demo

Decals:
  https://tuket.github.io/posts/2022-08-05-explosion-decals/

Falconeer clouds:
  https://youtu.be/5d8tx6K6hkk?t=6896
  spheres, noise texture, fresnel effect to hide outside edges
  tri-planar mapping to ea sphere
    e.g. tri-planar: https://gamedevelopment.tutsplus.com/articles/use-tri-planar-texture-mapping-for-better-terrain--gamedev-13821
  clouds are lit by using the skybox gradient (not lit on sphere)
    "if i were to turn off the distortion, they would disappear into the sky"
  sorting and clipping isn't an issue since clouds are so similar w/ tri-planar mapping
  still does alpha sorting?

Falconeer floodlights:
  triangular prism,
  addative,
  fade out w/ depth blend so u don't get ugly intersections

Reternal particle system:
  This is a great talk about the custom particle system used in Reternal for everything incl tentacles
  https://www.youtube.com/watch?v=qbkb8ap7vts
  This could be a great starting point for our own particle system, and I think it fits nicely with the Cy workflow

  "Semi-Lagrangian grid-based fluid sim"
    centered around the player, running at all time
      simulates air flow
    particles can simple from it
      and affect it?

Twitter thread on downsampling, blur, and bloom:
  https://twitter.com/BartWronsk/status/1570399970406305800
  "
  Downsample = lowpass filter + decimate (take one pixel in N).
  Blur = lowpass filter.
  "
  what is Nyquist?
  https://bartwronski.com/2022/03/07/fast-gpu-friendly-antialiasing-downsampling-filter/

Ratchet and Clank team on lighting:
  https://www.youtube.com/watch?v=geErfczxwjc
  exposure is super important to get right

"Let's talk about different ways of providing per draw data. What are their CPU and GPU performance implications?

UBOs, instance rate VB, push constants...":
  https://twitter.com/SebAaltonen/status/1573927036015546369

Shadow maps take 2 by ThinMatrix:
  https://www.youtube.com/watch?v=uueB2kVvbHo
  Only render backfaces in shadow pass
    challange: little light gaps at the seems
  use percent-closer filtering + random-ish offsets
    ideally circular; ideally comparison-filter (?)
  update shadow map region based on camera

HypeHype and Blender guys talking about line rendering:
  https://twitter.com/hypersomniac_/status/1575776294096490496
  https://twitter.com/SebAaltonen/status/1575778699378675712

Recommendations for learning graphics programming:
  https://medium.com/vrtigo/resources-for-beginning-graphics-programming-c0da724381bc
  https://interplayoflight.wordpress.com/2018/07/08/how-to-start-learn-graphics-programming/
  https://erkaman.github.io/posts/beginner_computer_graphics.html
  https://www.realtimerendering.com
  https://foundationsofgameenginedev.com

Frustum culling:
  https://bruop.github.io/frustum_culling/
  better, seperating axis theorem:
    https://gamedev.stackexchange.com/questions/44500/how-many-and-which-axes-to-use-for-3d-obb-collision-with-sat/44501#44501

To sleuth:
  https://www.reedbeta.com

Bundling + frustum culling?
  https://computergraphics.stackexchange.com/questions/13021/how-to-do-frustum-culling-with-draw-call-bundling
  https://computergraphics.stackexchange.com/questions/4066/whats-the-main-difference-of-pipeline-process-between-vulkan-and-dx12
  secondary command buffers (vulkan):
    https://community.khronos.org/t/what-are-the-advantages-of-secondary-command-buffers/6977/5
  dx12 vs vulkan:
    https://computergraphics.stackexchange.com/questions/4066/whats-the-main-difference-of-pipeline-process-between-vulkan-and-dx12

"GPU-driven rendering":
  Offload culling and LOD selection to GPU?
  https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf

Links from @reedbeta:
    "Domipheus Labs", "Domipheus Labs", http://labs.domipheus.com/blog/feed/, http://labs.domipheus.com/blog
    .mischief.mayhem.soap., .mischief.mayhem.soap., http://msinilo.pl/blog/?feed=rss2, http://msinilo.pl/blog2/
    0 FPS, 0 FPS, https://0fps.net/feed/, https://0fps.net
    A Graphics guy's note, A Graphics guy's note, https://agraphicsguy.wordpress.com/feed/, https://agraphicsguy.wordpress.com
    A Random Walk Through Geek-Space, A Random Walk Through Geek-Space, http://www.sebastiansylvan.com/index.xml, https://www.sebastiansylvan.com/
    Accidentally Quadratic, Accidentally Quadratic, http://accidentallyquadratic.tumblr.com/rss, https://accidentallyquadratic.tumblr.com/
    Alexandre Pestana, Alexandre Pestana, http://www.alexandre-pestana.com/feed/, http://www.alexandre-pestana.com
    Algorithmic Assertions, Algorithmic Assertions, http://algassert.com/feed.xml, http://algassert.com
    Andreas Fredriksson, Andreas Fredriksson, http://deplinenoise.wordpress.com/feed/, https://deplinenoise.wordpress.com
    Andrzej's C++ blog, Andrzej's C++ blog, http://akrzemi1.wordpress.com/feed/, https://akrzemi1.wordpress.com
    Angelo Pesce, Angelo Pesce, http://c0de517e.blogspot.com/feeds/posts/default, http://c0de517e.blogspot.com/
    Anteru's blog, Anteru's blog, https://anteru.net/rss.xml, https://anteru.net/
    Apoorva Joshi, Apoorva Joshi, http://apoorvaj.io/feed.xml, http://apoorvaj.io
    Aras' website, Aras' website, http://aras-p.info/atom.xml, https://aras-p.info/
    Articles by Eric Arnebäck, Articles by Eric Arnebäck, https://erkaman.github.io/rss.xml, https://erkaman.github.io/articles.html
    Arvid Gerstmann, Arvid Gerstmann, https://arvid.io/rss/, https://arvid.io/
    Attila Áfra, Attila Áfra, http://voxelium.wordpress.com/feed/, https://voxelium.wordpress.com
    Augmented Pixels, Augmented Pixels, https://www.tobias-franke.eu/log/rss/index.xml, https://www.tobias-franke.eu/
    Avoyd devlog, Avoyd devlog, http://www.enkisoftware.com/rss, https://www.enkisoftware.com/
    Bart Wronski, Bart Wronski, https://bartwronski.com/feed/, https://bartwronski.com
    Bit Bashing, Bit Bashing, https://bitbashing.io/feed.xml, https://bitbashing.io/
    bitsquid: development blog, bitsquid: development blog, http://bitsquid.blogspot.com/feeds/posts/default, http://bitsquid.blogspot.com/
    Blog - Binomial, Blog - Binomial, http://www.binomial.info/blog?format=RSS, http://www.binomial.info/blog/
    Blog - Stephanie Hurlburt, Blog - Stephanie Hurlburt, http://stephaniehurlburt.com/blog?format=RSS, http://stephaniehurlburt.com/blog/
    Brian Karis, Brian Karis, http://graphicrants.blogspot.com/feeds/posts/default, http://graphicrants.blogspot.com/
    Brian Sharpe, Brian Sharpe, http://briansharpe.wordpress.com/feed/, https://briansharpe.wordpress.com
    Bruce Dawson, Bruce Dawson, http://randomascii.wordpress.com/feed/, https://randomascii.wordpress.com
    C++ Secrets, C++ Secrets, http://cppsecrets.blogspot.com/feeds/posts/default, http://cppsecrets.blogspot.com/
    Casey Muratori's Blog, Casey Muratori's Blog, http://mollyrocket.com/casey/stream_atom.rss, https://caseymuratori.com/blog
    Cass Everitt, Cass Everitt, http://blog.xyzw.us/feeds/posts/default?alt=rss, http://blog.xyzw.us/
    Casual Effects, Casual Effects, http://casual-effects.blogspot.com/feeds/posts/default, http://casual-effects.blogspot.com/
    cbloom rants, cbloom rants, http://cbloomrants.blogspot.com/feeds/posts/default, http://cbloomrants.blogspot.com/
    Christer Ericson, Christer Ericson, http://realtimecollisiondetection.net/blog/?feed=rss2, http://realtimecollisiondetection.net/blog
    Christian Schüler, Christian Schüler, http://www.thetenthplanet.de/feed, http://www.thetenthplanet.de
    Code &amp; Visuals, Code &amp; Visuals, http://blog.yiningkarlli.com/feeds/posts/default/, http://blog.yiningkarlli.com/
    Code Capsule, Code Capsule, http://codecapsule.com/feed/, http://codecapsule.com
    code4k, code4k, http://code4k.blogspot.com/feeds/posts/default, http://code4k.blogspot.com/
    Coded Interactions, Coded Interactions, https://blog.dimitridiakopoulos.com/rss/, https://blog.dimitridiakopoulos.com/
    CodeItNow, CodeItNow, http://www.rorydriscoll.com/feed/, http://www.rorydriscoll.com
    codersnotes.com, codersnotes.com, http://www.codersnotes.com/rss/, http://www.codersnotes.com
    Coding Horror, Coding Horror, http://feeds.feedburner.com/codinghorror/, https://blog.codinghorror.com/
    Colour Science, Colour Science, https://www.colour-science.org/rss.xml, https://www.colour-science.org/
    Computing and Recording, Computing and Recording, https://computingandrecording.wordpress.com/feed/, https://computingandrecording.wordpress.com
    copypastepixel, copypastepixel, http://copypastepixel.blogspot.com/feeds/posts/default, http://copypastepixel.blogspot.com/
    Cyril Crassin, Cyril Crassin, https://blog.icare3d.org/feeds/posts/default, https://blog.icare3d.org/
    Dan Luu, Dan Luu, http://danluu.com/atom.xml, http://danluu.com/
    Dev Curious - Medium, Dev Curious - Medium, https://blog.forrestthewoods.com/feed, https://blog.forrestthewoods.com?source=rss----4e87161953d9---4
    Dreams of flashy pixels (on a white website), Dreams of flashy pixels (on a white website), https://flashypixels.wordpress.com/feed/, https://flashypixels.wordpress.com
    duangle, duangle, https://blog.duangle.com/feeds/posts/default, https://blog.duangle.com/
    Embedded in Academia, Embedded in Academia, http://blog.regehr.org/feed, https://blog.regehr.org
    Emily Price, Emily Price, https://emilypriceisright.com/feed/, https://emilypriceisright.com
    EpicShaders, EpicShaders, http://www.epicshaders.com/feed/, https://www.epicshaders.com
    Extreme Learning, Extreme Learning, http://extremelearning.com.au/feed/, http://extremelearning.com.au
    Fabian Giesen, Fabian Giesen, http://fgiesen.wordpress.com/feed/, https://fgiesen.wordpress.com
    Fabien Sanglard, Fabien Sanglard, http://fabiensanglard.net/rss.xml, http://fabiensanglard.net
    Filmic Worlds, Filmic Worlds, http://filmicworlds.com/feed.xml, http://filmicworlds.com/
    Forward Scattering - The Weblog of Nicholas Chapman, Forward Scattering - The Weblog of Nicholas Chapman, http://www.forwardscattering.org/rss, http://www.forwardscattering.org/
    fuzzy notepad, fuzzy notepad, http://eev.ee/feeds/atom.xml, https://eev.ee/
    Fuzzy Reflection, Fuzzy Reflection, https://fuzzyreflection.com/feed/, https://fuzzyreflection.com
    Gaffer On Games, Gaffer On Games, http://new.gafferongames.com/index.xml, https://gafferongames.com/
    Gaming Reality, Gaming Reality, http://www.petecollier.com/?feed=rss2, http://www.petecollier.com
    Gazoo.vrv, Gazoo.vrv, http://donw.io/index.xml, http://donw.io/
    General Protection Fault, General Protection Fault, https://gpfault.net/rss.xml, http://www.gpfault.net/
    Giliam de Carpentier, Giliam de Carpentier, http://www.decarpentier.nl/feed, http://www.decarpentier.nl
    ginsweater's blog, ginsweater's blog, http://ginsweater.com/blog/feed/, http://ginsweater.com/blog
    glowybits, glowybits, http://www.glowybits.com/index.xml, http://www.glowybits.com/index.xml
    GPU Dissector, GPU Dissector, http://gpudissector.blogspot.com/feeds/posts/default, http://gpudissector.blogspot.com/
    GPU Pro, GPU Pro, http://gpupro.blogspot.com/feeds/posts/default, http://gpupro.blogspot.com/
    GPU Zen, GPU Zen, http://gpuzen.blogspot.com/feeds/posts/default?alt=rss, http://gpuzen.blogspot.com/
    Graham Wihlidal, Graham Wihlidal, https://www.wihlidal.com/feed.xml, https://www.wihlidal.com/
    Humus, Humus, http://www.humus.name/rss.xml, http://www.humus.name
    I'm doing it wrong, I'm doing it wrong, http://imdoingitwrong.wordpress.com/feed/, https://imdoingitwrong.wordpress.com
    Ignacio Castaño, Ignacio Castaño, http://castano.ludicon.com/blog/feed/, http://www.ludicon.com/castano/blog
    Imagination, Imagination, https://imgtec.com/feed/, https://www.imgtec.com
    Industrial Arithmetic, Industrial Arithmetic, http://industrialarithmetic.blogspot.com/feeds/posts/default, http://industrialarithmetic.blogspot.com/
    Infinite-Realities, Infinite-Realities, http://ir-ltd.net/feed/, http://ir-ltd.net
    James Dolan, James Dolan, http://jamesdolan.blogspot.com/feeds/posts/default, http://jamesdolan.blogspot.com/
    James McNeill, James McNeill, http://playtechs.blogspot.com/feeds/posts/default, http://playtechs.blogspot.com/
    Jendrik Illner - 3D Programmer on Jendrik Illner - 3D Programmer, Jendrik Illner - 3D Programmer on Jendrik Illner - 3D Programmer, https://www.jendrikillner.com/index.xml, https://www.jendrikillner.com/
    Jeroen Baert, Jeroen Baert, https://www.forceflow.be/feed/, https://www.forceflow.be
    Joe Duffy's Blog, Joe Duffy's Blog, http://joeduffyblog.com/feed.xml, http://joeduffyblog.com/
    Johan Andersson, Johan Andersson, http://repi.blogspot.com/feeds/posts/default, http://repi.blogspot.com/
    John Ahlgren, John Ahlgren, http://john-ahlgren.blogspot.com/feeds/posts/default, http://john-ahlgren.blogspot.com/
    John Calsbeek, John Calsbeek, http://www.johncalsbeek.com/feed.xml, http://www.johncalsbeek.com
    John White, John White, http://johnwhite3d.blogspot.com/feeds/posts/default, http://johnwhite3d.blogspot.com/
    Jon Olick - Home, Jon Olick - Home, https://www.jonolick.com/2/feed, https://www.jonolick.com/home
    Joost's Dev Blog, Joost's Dev Blog, http://joostdevblog.blogspot.com/feeds/posts/default, http://joostdevblog.blogspot.com/
    Jorge Jimenez, Jorge Jimenez, http://www.iryoku.com/feed, http://www.iryoku.com
    Julia Evans, Julia Evans, http://jvns.ca/atom.xml, http://jvns.ca
    Kate Murphy, Kate Murphy, https://kate.io/feed.xml, https://kate.io/
    Kosmokleaner, Kosmokleaner, http://kosmokleaner.wordpress.com/feed/, https://kosmokleaner.wordpress.com
    kosmonaut's blog, kosmonaut's blog, http://kosmonautblog.wordpress.com/feed/, https://kosmonautblog.wordpress.com
    Kostas Anagnostou, Kostas Anagnostou, http://interplayoflight.wordpress.com/feed/, https://interplayoflight.wordpress.com
    Krister Walfridsson’s blog, Krister Walfridsson’s blog, https://kristerw.blogspot.com/feeds/posts/default, https://kristerw.blogspot.com/
    Krzysztof Narkowicz, Krzysztof Narkowicz, http://knarkowicz.wordpress.com/feed/, https://knarkowicz.wordpress.com
    Lei Zhang (antiagainst)'s Blog, Lei Zhang (antiagainst)'s Blog, https://antiagainst.github.io/index.xml, https://antiagainst.github.io/
    Lia-Sae's Blog, Lia-Sae's Blog, http://feeds.lia-sae.net/main.rss.xml, http://lia-sae.net/
    Light is beautiful, Light is beautiful, http://lousodrome.net/blog/light/feed/, http://lousodrome.net/blog/light
    Maister's Graphics Adventures, Maister's Graphics Adventures, http://themaister.net/blog/feed/, http://themaister.net/blog
    marc-b-reynolds.github.io, marc-b-reynolds.github.io, http://marc-b-reynolds.github.io/feed.xml, http://marc-b-reynolds.github.io
    Marcelo's WebLog, Marcelo's WebLog, http://blogs.msdn.com/marcelolr/rss.xml, https://blogs.msdn.microsoft.com/marcelolr
    Marmakoide's Blog, Marmakoide's Blog, http://blog.marmakoide.org/?feed=rss2, http://blog.marmakoide.org
    Matt Pettineo, Matt Pettineo, http://mynameismjp.wordpress.com/feed/, https://mynameismjp.wordpress.com
    Matt Pharr’s blog, Matt Pharr’s blog, http://pharr.org/matt/blog/feed.xml, https://pharr.org/matt/blog/
    Matt Swoboda, Matt Swoboda, http://directtovideo.wordpress.com/feed/, https://directtovideo.wordpress.com
    me == DeanoC, me == DeanoC, http://deanoc.com/rss.xml, http://deanoc.com/
    Mediocre game technology, Mediocre game technology, https://tuxedolabs.blogspot.com/feeds/posts/default?alt=rss, https://tuxedolabs.blogspot.com/
    Michal Valient, Michal Valient, http://www.dimension3.sk/feed/, http://www.dimension3.sk
    Miles Macklin, Miles Macklin, http://blog.mmacklin.com/feed/, http://blog.mmacklin.com
    Ming-Lun "Allen" Chou | 周明倫, Ming-Lun "Allen" Chou | 周明倫, http://allenchou.net/feed/, http://allenchou.net
    Molecular Musings, Molecular Musings, http://molecularmusings.wordpress.com/feed/, https://blog.molecular-matters.com
    Moments in Graphics, Moments in Graphics, http://momentsingraphics.de/?feed=rss2, http://momentsingraphics.de
    Morten Mikkelsen, Morten Mikkelsen, http://mmikkelsen3d.blogspot.com/feeds/posts/default, http://mmikkelsen3d.blogspot.com/
    Mr F, Mr F, http://ndotl.wordpress.com/feed/, https://ndotl.wordpress.com
    Nathan Reed’s coding blog, Nathan Reed’s coding blog, http://reedbeta.com/feed/, http://reedbeta.com/
    njs blog, njs blog, https://vorpus.org/blog/feeds/atom.xml, https://vorpus.org/blog/
    nlguillemot, nlguillemot, https://nlguillemot.wordpress.com/feed/, https://nlguillemot.wordpress.com
    NVIDIA Developer Blog, NVIDIA Developer Blog, http://developer.nvidia.com/blog/feed/2, https://devblogs.nvidia.com
    OpenGL SuperBible, OpenGL SuperBible, http://www.openglsuperbible.com/feed/, http://www.openglsuperbible.com
    Our Machinery, Our Machinery, https://ourmachinery.com/index.xml, https://ourmachinery.com/
    Outerra, Outerra, https://outerra.blogspot.com/feeds/posts/default, https://outerra.blogspot.com/
    Pablo Zurita's blog, Pablo Zurita's blog, http://pzurita.wordpress.com/feed/, https://pzurita.wordpress.com
    Pete Shirley's Graphics Blog, Pete Shirley's Graphics Blog, http://psgraphics.blogspot.com/feeds/posts/default, http://psgraphics.blogspot.com/
    Peter Sikachev Dev Blog, Peter Sikachev Dev Blog, http://petersikachev.blogspot.com/feeds/posts/default?alt=rss, http://petersikachev.blogspot.com/
    pixel jet stream, pixel jet stream, https://pixeljetstream.blogspot.com/feeds/posts/default, https://pixeljetstream.blogspot.com/
    Pointers Gone Wild, Pointers Gone Wild, http://pointersgonewild.com/feed/, https://pointersgonewild.com
    Preshing on Programming, Preshing on Programming, https://preshing.com/feed, https://preshing.com/
    Programming in the 21st Century, Programming in the 21st Century, http://prog21.dadgum.com/atom.xml, http://prog21.dadgum.com/
    Promit's Ventspace, Promit's Ventspace, http://ventspace.wordpress.com/feed/, https://ventspace.wordpress.com
    Pronounced /ˈʃɪ.vəʊ.æ/, Pronounced /ˈʃɪ.vəʊ.æ/, https://blog.shivoa.net/feeds/posts/default, https://blog.shivoa.net/
    Rambles about code, games and other things, Rambles about code, games and other things, http://chainedchaos31.tumblr.com/rss, http://chainedchaos31.tumblr.com/
    Randy Gaul's Game Programming Blog, Randy Gaul's Game Programming Blog, http://www.randygaul.net/feed/, http://www.randygaul.net
    Ray Tracey's blog, Ray Tracey's blog, http://raytracey.blogspot.com/feeds/posts/default, http://raytracey.blogspot.com/
    Real-Time Rendering, Real-Time Rendering, http://www.realtimerendering.com/blog/feed/, http://www.realtimerendering.com/blog
    Real-Time Voxels, Real-Time Voxels, http://realtimevoxels.blogspot.com/feeds/posts/default, http://realtimevoxels.blogspot.com/
    Rendering Equations, Rendering Equations, https://schuttejoe.github.io/index.xml, https://schuttejoe.github.io/
    Rich Geldreich's Tech Blog, Rich Geldreich's Tech Blog, http://richg42.blogspot.com/feeds/posts/default, http://richg42.blogspot.com/
    Run Hello, Run Hello, http://msm.grumpybumpers.com/?feed=rss2, https://msm.runhello.com
    Sébastien Lagarde, Sébastien Lagarde, http://seblagarde.wordpress.com/feed/, https://seblagarde.wordpress.com
    Sam Martin, Sam Martin, http://www.palgorithm.co.uk/feed/, http://www.palgorithm.co.uk
    Sanders' blog, Sanders' blog, https://sandervanrossen.blogspot.com/feeds/posts/default, https://sandervanrossen.blogspot.com/
    Shiny Pixels, Shiny Pixels, http://vec3.ca/feed/, http://vec3.ca
    Simon schreibt., Simon schreibt., http://simonschreibt.blogspot.com/feeds/posts/default, http://simonschreibt.blogspot.com/
    Simon's Graphics Blog, Simon's Graphics Blog, http://sjbrown.co.uk/index.xml, http://sjbrown.co.uk/
    Snorri Sturluson, Snorri Sturluson, https://snorristurluson.github.io/feed.xml, https://snorristurluson.github.io//
    Solid Angle, Solid Angle, http://solid-angle.blogspot.com/feeds/posts/default, https://solid-angle.blogspot.com/
    Sophie's Blog, Sophie's Blog, http://www.sophiehoulden.com/feed/, http://www.sophiehoulden.com
    Stephen Hill, Stephen Hill, http://blog.selfshadow.com/feed/, http://blog.selfshadow.com/
    Steve McAuley, Steve McAuley, http://blog.stevemcauley.com/feed/, http://blog.stevemcauley.com
    Stevey's Blog Rants, Stevey's Blog Rants, http://steve-yegge.blogspot.com/feeds/posts/default, http://steve-yegge.blogspot.com/
    Stories by April Wensel on Medium, Stories by April Wensel on Medium, https://medium.com/feed/@Aprilw, https://medium.com/@Aprilw?source=rss-e52a7bee92a7------2
    Stories by Ben Golus on Medium, Stories by Ben Golus on Medium, https://medium.com/feed/@bgolus, https://medium.com/@bgolus?source=rss-d4d33ac3d661------2
    Stories by Fred Jennings on Medium, Stories by Fred Jennings on Medium, https://medium.com/feed/@Esquiring, https://medium.com/@Esquiring?source=rss-91833865fa3f------2
    Stories by Steve Yegge on Medium, Stories by Steve Yegge on Medium, https://medium.com/feed/@steve.yegge, https://medium.com/@steve.yegge?source=rss-c1ec701babb7------2
    Syntopia, Syntopia, http://blog.hvidtfeldts.net/index.php/feed/, http://blog.hvidtfeldts.net
    The Big Mud Puddle, The Big Mud Puddle, http://evincarofautumn.blogspot.com/feeds/posts/default, http://evincarofautumn.blogspot.com/
    The blog at the bottom of the sea, The blog at the bottom of the sea, http://blog.demofox.org/feed/, https://blog.demofox.org
    The blog of Kyle Halladay, The blog of Kyle Halladay, http://kylehalladay.com/atom.xml, http://kylehalladay.com
    The Brain Dump, The Brain Dump, http://floooh.github.com/feed.xml, https://floooh.github.com/
    The Burning Basis Vector, The Burning Basis Vector, http://www.joshbarczak.com/blog/?feed=rss2, http://www.joshbarczak.com/blog
    The Code Deposit, The Code Deposit, http://codedeposit.blogspot.com/feeds/posts/default, http://codedeposit.blogspot.com/
    The Hacks of Life, The Hacks of Life, http://hacksoflife.blogspot.com/feeds/posts/default, http://hacksoflife.blogspot.com/
    The Legend of GameDev, The Legend of GameDev, http://david.fancyfishgames.com/feeds/posts/default, http://david.fancyfishgames.com/
    The Old New Thing, The Old New Thing, http://blogs.msdn.com/oldnewthing/rss.xml, https://blogs.msdn.microsoft.com/oldnewthing
    The Visual Studio Blog, The Visual Studio Blog, http://blogs.msdn.com/b/visualstudio/rss.aspx, https://blogs.msdn.microsoft.com/visualstudio
    The Witness, The Witness, http://the-witness.net/news/feed/, http://the-witness.net/news
    Tom Bissell, Tom Bissell, http://grantland.com/contributors/tom-bissell/feed/, http://grantland.com
    Tom Forsyth, Tom Forsyth, http://tomforsyth1000.github.io/blog.wiki.xml, http://eelpi.gotdns.org/blog.wiki.html
    Tom Hammersley, Tom Hammersley, http://tomhammersley.blogspot.com/feeds/posts/default, http://tomhammersley.blogspot.com/
    Tony Albrecht, Tony Albrecht, http://seven-degrees-of-freedom.blogspot.com/feeds/posts/default, http://seven-degrees-of-freedom.blogspot.com/
    TRYING TO FIND THE OBVIOUS, TRYING TO FIND THE OBVIOUS, http://www.hlsl.co.uk/blog?format=RSS, http://www.hlsl.co.uk/
    Video Encoding &amp; Streaming Technologies, Video Encoding &amp; Streaming Technologies, http://sonnati.wordpress.com/feed/, https://sonnati.wordpress.com
    Wicked Engine DevBlog, Wicked Engine DevBlog, https://turanszkij.wordpress.com/feed/, https://turanszkij.wordpress.com
    Will push pixels for food, Will push pixels for food, http://www.gijskaerts.com/wordpress/?feed=rss2, http://www.gijskaerts.com/wordpress
    Windows Subsystem for Linux, Windows Subsystem for Linux, https://blogs.msdn.microsoft.com/wsl/feed/, https://blogs.msdn.microsoft.com/wsl
    Wolfgang Engel, Wolfgang Engel, http://diaryofagraphicsprogrammer.blogspot.com/feeds/posts/default, http://diaryofagraphicsprogrammer.blogspot.com/
    WORK: A Guide, WORK: A Guide, https://www.workaguide.com/?format=rss, https://www.workaguide.com/
    xoofx, xoofx, http://xoofx.com/feed.xml, http://xoofx.com/
    Yosoygames, Yosoygames, http://yosoygames.com.ar/wp/feed/, http://www.yosoygames.com.ar/wp
    Yuriy O'Donnell, Yuriy O'Donnell, http://kayru.org/feed.xml, http://kayru.org/
    Zack Rusin, Zack Rusin, http://zrusin.blogspot.com/feeds/posts/default, http://zrusin.blogspot.com/
    Zero Wind :: Jamie Wong, Zero Wind :: Jamie Wong, http://feeds.feedburner.com/JamieWong, http://jamie-wong.com/
    zeuxcg.org, zeuxcg.org, http://zeuxcg.org/feed/, http://zeuxcg.org
    ZigguratVertigo's Hideout, ZigguratVertigo's Hideout, http://zigguratvertigo.com/feed/, https://colinbarrebrisebois.com

Algebraic Geometry for Computer Graphics:
  https://courses.cs.washington.edu/courses/cse590b/13au/
  hn commenter recommended:
    http://www.cs.uu.nl/docs/vakken/magr/2021-2022/index.html

Wind on terrain simulation:
  https://nickmcd.me/2022/10/01/procedural-wind-and-clouds-using-gpu-accelerated-lattice-boltzmann-method/

Indirect draw in WebGPU?
  https://github.com/gpuweb/gpuweb/issues/2189
  https://gpuweb.github.io/gpuweb/#dom-gpurendercommandsmixin-drawindirect

Backstory on splitting render passes?
  https://github.com/gpuweb/gpuweb/issues/2046#issuecomment-909981545

Regarding Nanite:
  https://golden-mandolin-675.notion.site/Notes-about-Nanite-in-UE5-1-preview-1bd49cb5986a46629214b82f9ce58ae5
  https://www.youtube.com/watch?v=eviSykqSUUw
  https://docs.unrealengine.com/5.0/en-US/nanite-virtualized-geometry-in-unreal-engine/
    meshes are analyzed and broken down into hierarchical clusters of triangle groups.
    clusters are swapped on the fly at varying levels of detail based on the camera view, and connect perfectly without cracks to neighboring clusters within the same object. Data is streamed in on demand so that only visible detail needs to reside in memory. Nanite runs in its own rendering pass that completely bypasses traditional draw calls.
    Nanite is currently limited to rigid meshes
    "the maximum number of instances that can be present in the scene is hard-locked to 16 million instances"
      uint24 ?
      "Only instances streamed in are counted towards the total."
  https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf
    considered:
      voxels
        doesn't work well for hard surfaces, animation, and uses too much space
        it's a form of uniform resampling, which will never work
      subdivision Surfaces
        doesn't make things low-poly enough
      displacement maps
        doesn't work on bike chain, can't increase genus of a surface
         it's a form of uniform resampling
      Geometry images, 
        same as displacement maps
      Point rendering
        super fast (?)
        requires hole filling, a basically unsolvable problem
          intentional gap or hole?
        needs extra connectivity data, aka triangle index buffer (lol)
        seems ripe for ML
        Denoising is a form of sparse data interpolation
      triangles
        "for our requirements have found no higher quality or faster solution than triangles."
        "If you can build an art style around the aesthetics of one of these other representations and embrace their pros and cons they could be incredibly valuable but Unreal can’t impose an art style."
      Renderer now retained mode
        ● GPU scene representation persists across frames
        ● Sparsely updated where things change
        ● All vertex/index data in single large resource
      Per view:
        ● GPU instance cull
        ● Triangle rasterization
      If only drawing depth the entire scene can draw with 1 DrawIndirect
      unreal has a "retained mode design"
      mesh data is stored in single large resources.
      This means we can touch any of it at once without requiring bindless resources to do so.
      necessary building blocks for a GPU driven pipeline.
      Group triangles into clusters
        ● Build bounding data for each cluster
      Cull clusters based on bounds 
        ● Frustum cull
        ● Occlusion cull
          Hierarchical Z-Buffer (HZB)
      We are all familiar with cluster culling made popular by [40] and, [41] but the idea goes way back, at least to 1996 [37]
      "We have found cone based backface culling is not typically worthwhile"
      occlusion culling:
        The test is identical to [39].
        Visible objects last frame are likely still visible this frame
        Two pass:
          Draw what was visible in the previous frame
          Build HZB
          Draw what is visible now but wasn’t in the last frame
          Conservative, Only falls apart under extreme visibility changes
      First GPU occlusion: “March of the Froblins” [38]
      First two-pass occlusion: “Patch-based Occlusion Culling for Hardware Tessellation” [19]
      "Decouple visibility from material"
        want to eliminate:
          Switching shaders during rasterization
          Overdraw for material eval
          Depth prepass to avoid overdraw
          Pixel quad inefficiencies from dense meshes
        options:
          object space shading:
            REYES
            Texture space shading
            but these overshade by 4x or more
              maybe solvable through caching but: view dependent, animating, non UV based, materials r too common
          Deferred materials
      Visibility Buffer
        Write geometry data to screen:
          Depth, InstanceID, TriangleID
        Material shader per pixel:
          Load VisBuffer
          Load instance transform
          Load 3 vert indexes
          Load 3 positions
          Transform positions to screen
          Derive barycentric coordinates (?TO LEARN) for pixel
          Load and lerp attributes
        Sounds crazy? Not as slow as it seems
          Lots of cache hits
          No overdraw or pixel quad inefficiencies
        Material pass writes GBuffer
          Integrates with rest of our deferred shading renderer
        Now we can draw all opaque geometry with 1 draw
          Completely GPU driven
          Not just depth prepass
          Rasterize triangles once per view
      "CPU cost is independent from number of objects in the scene or in view."
      "Materials are a draw per shader"
      "rasterize triangles once per view"
      Linear scaling in instances can be ok
        "We can handle a million instances easily."
      Linear scaling in triangles is not ok
      "Memory bandwidth is not a good vector to lean on."
      the cost of rendering geometry should scale with screen resolution, not scene complexity.
      hierarchy of LODs, Simplest is tree of clusters
        Parents are the simplified versions of their children
      view dependent cut of LOD tree
        "based on the screen space projected error of the cluster."
      Request data on demand during rendering
        Like virtual texturing
      Cracks!
        Naive solution: Lock shared boundary edges during simplification
          Collects dense cruft, Especially between deep subtrees
      Cracks solution:
        Can detect these cases during build
        Group clusters
          Force them to make the same LOD decision
          Now free to unlock shared edges and collapse them
        The key idea is to alternate group boundaries from level to level by grouping different clusters.
        What is a boundary in one level becomes the interior in the next level
    It wasn’t until years later after partly implementing QuickVDR that I tried rereading it once again and it sunk in how insightful it really was as a super set of multiple schemes.
      I highly recommend reading this dissertation by Federico Ponchio https://d-nb.info/997062789/34 if the original paper doesn’t make sense at first. The slides for the paper are also helpful http://vcg.isti.cnr.it/Publications/2005/CGGMPS05/Slide_BatchedMT_Vis05.pdf
    "we want groups of multiples of 128 triangles such that they can be divided into clusters of exactly 128."
     Build operations
      Cluster original triangles
      While NumClusters > 1
        Group clusters to clean their shared boundary
        Merge triangles from group into shared list
        Simplify to 50% the # of triangles
        Split simplified triangle list into clusters (128 tris)
    Group those with the most shared boundary edges ● Less boundary edges == less locked edges
      This problem is called graph partitioning
      Partition a graph optimizing for minimum edge cut cost
      "Use METIS library to solve"
    TODO: impl http://gamma.cs.unc.edu/QVDR/
    Mesh simplify
      Edge collapsing
        "typical edge collapsing decimation"
      Picks smallest error edge first
      Error calculated using Quadric Error Metric (QEM)
      Optimizes position of new vertex for minimal error
      Highly refined
      Returns estimate of error introduced
        Later projected on screen to number of pixels error
        The hardest part!
      "So I calculate the average surface area per triangle and rescale the mesh such that that is roughly a constant I chose as a good mix of position to attribute balance."
      "That small guy on the shoulder of the big guy is actually a really important case to get right."
    Runtime LOD cutting on cluster DAG:
      parallel on GPU, no communication or expansion
      Draw a cluster when:
        Parent error is too high &&
        Our error is small enough
      only works if there is a unique cut,
        so force error to be monotonic
    No LOD popping b/c of:
      < 1 pixel error
        between parent and child LOD?
        "This is the reason why getting an accurate error estimate is so important."
      (temporal antialiasing) TAA
    When can we LOD cull a cluster?
      Render: ParentError > threshold && ClusterError <= threshold
      Cull: ParentError <= threshold || ClusterError > threshold
        "we build a BVH (bounding volume hierarchy?) over the clusters."
          "Traversing this tree is a classic parallel expansion work scheduling problem."
          Persistent threads model:
            Can’t spawn new threads. Reuse them instead!
            Manage our own job queue
            Single dispatch with enough worker threads to fill GPU
            "Use simple multi-producer multi-consumer (MPMC) job-queue to communicate between threads"
    TOLEARN: "Cluster selection must be isotropic like mip selection"
      isotropic: "same in all orientations/directions"
    TODO: when rendering engineers say "vertex colors", what do they mean?
    My summary of what's novel about nanite:
      Better triangle cluster boundries than prior work
      Better LOD switching b/c of good error estimation
      Basically really good at solving cracks between clusteres of triangles w/ different LODs
      Lots of engineering throughout to make it practical
      Why now?
        SSDs are necessary,
        GPUs are powerful enough for a ton of per-pixel work
        There's a threshold in GPU and memory that needs to be crossed in order to get such scene complexity-invariant results
      Weakest at folliage like leaves and grass; still a big TODO for them
      

  Unanswered:
    https://gamedev.stackexchange.com/questions/198454/how-does-unreal-engine-5-nanite-works
      
to read, nanite guys: 
  https://www.wihlidal.com/blog/
  http://graphicrants.blogspot.com

Nanite frame analysis:
  https://www.elopezr.com/a-macro-view-of-nanite/

HZB:
  https://docs.unrealengine.com/4.26/en-US/RenderingAndGraphics/VisibilityCulling/

HiZ culling:
  https://www.rastergrid.com/blog/2010/10/hierarchical-z-map-based-occlusion-culling/

Unreal Lumen:
  https://knarkowicz.wordpress.com/2022/08/18/journey-to-lumen/
  http://advances.realtimerendering.com/s2022/index.html#Lumen

HPG panel talk:
  https://youtu.be/CoGo_3C7xR0?t=14108

ZeniMax guy blog:
  https://alextardif.com/eden.html

WickedEngine:
  https://wickedengine.net

"what a "minimum viable" gpu api would look like":
  https://twitter.com/nice_byte/status/1579187394880090112
  "
  dynamic rendering is very good. toss renderpasses, subpasses and framebuffers into the trash bin.
  programmable vertex pulling only.
  fully bindless. no binding images, buffers or anything else, ever. toss descriptor sets to the same pile as renderpasses.
  in fact, no "buffers". preallocate chunks of different types of memory (e.g. for constants, for vert data), expose them to shaders and let the shader figure it out.
  eliminate the concept of "samplers" from cpu code.  shaders have an indexable table of predefined samplers, and use them as necessary.
  no msaa. everyone's doing taa these days anyway
  do something so that the application knows what kinds of memory it's going to need _before_ it starts creating resources. right now in vk you don't know which heap you want until you actually create an image or buffer.
  synchronization can probably be kept at api level. if you write your code carefully you can keep it to a minimum anyway.
  "

TAA (temporal anti-aliasing):
  https://en.wikipedia.org/wiki/Temporal_anti-aliasing
  seems like everybody uses this; more performant than MSAA?
  Brief overview of AA: https://www.youtube.com/watch?v=g8iliJFLHSQ
  TAA impl in INSIDE: https://www.youtube.com/watch?v=2XXS5UyNjjU
  Brian Karis re TAA: https://twitter.com/BrianKaris/status/1327712610364522496

  "Optical flow. :) I’d very seriously consider it. ILK can be super fast on the GPU."
    ?

"Monte Carlo integration of the rendering equation":
  What is it?
  https://twitter.com/BrianKaris/status/1327713322184019968?s=20&t=i_47-O6x15oB2ujl1OQjOA
  
  https://www.cs.rpi.edu/~cutler/classes/advancedgraphics/S08/lectures/17_monte_carlo.pdf

TODO: Dreams' point-based renderer
  "atomic scattering points" ?
  "vray proxies", "grave hope"
  infinitesimal points
    fast scatter through binning
      64bit atomics
    holes and AA
    thickening problem
    prefiltered AA doesn't work
    binning points === nearest neighbor filtering

Surfels:
  https://en.wikipedia.org/wiki/Surfel
  An object is represented by a dense set of points or viewer-facing discs holding lighting information
  https://www.youtube.com/watch?v=h1ocYFrtsM4

Dreams:
  https://www.youtube.com/watch?v=u9KNtnCZDMI
  Alex evens
  no texture, models, meshes
  art style + toolset + engine
  tech direection + art direction
  1. anton's blob editor
    exp with distance fields
  everything out of list of edits, distance field
  "real csg"
  operations: add, subtract, color
  tried: rendering via deforming low poly meshes
  tried: animate by moving edits around
    as edit list grows, cost grows
  plan: edit list -> (compute shader of doom) sparse distance field -> marching cubes in to mesh? OR filter into SVO? scatter into point cloud?
  eek: 1000x1000x1000x10000 edits
  split SDF fields up hierarchically
  SDF distances: "L2" distance doesn't work
  "efficient distance and efficient voxalization" ??
  Used the "max norm"
    not x^2+y^2+z^2
    instead max(abs(x), abs(y), abs(z))
  soft_min(a,b,r) {
    float e = max(r-abs(a,b),0);
    return min(a,b) - e*e*0.25f/r;
  } // must equal hard-min after distance r ?
  need to know how many edits contribute to a surface?
  how to render
    anton slide deck: "cultural learnings for making benefit glorious polygonalization of signed distance fields"
  marching cubes: v dense meshes
  looked at dual contouring of hermite data
    sucks when has to decide smooth vs hard surfaces
    introduces intersections
  intersection-free contouring on an octree grid
    becomes non-manifold (watertight)
  manifold dual contouring
    brings back intersections
  pick one: manifold or non-self-intersecting
  polygons r bad
    they are amazing as surface rep! but they're surprisingly hard.
    best rep of flat surface
  artists like bloby krinkly stuff, not natural for flat surfaces
  inspired by: "volumetric billboards" decaudin, fabrice neyre
    "someone should make an indie game"
    everything is voxels, but little fuzzy voxels,
    sheets of polygons, aligned with screen, cut through ur texture
    multiple overlapping objects, effectively order independent transparency
    mm siggraph slides have way simpler way to impl it
  gigavoxels
    sparse voxels
  engine 2: brick engine
    let's try a hybrid volumetric billboards / gigavoxels
    8x8x8 voxel blocks, LOD adjusted,
      parallax occlusion mapping
        outputs a depth per pixel
      from surface of cube to nice bumpy surface
    models look like untextured poly models
    really wanted OIT (order independent transparency): long rabbit hole 12-18mo
      walk cubes, accumulate fuzz on surface, output 
  argh:
    polygons? too hard
    volumetric billboards? too much filtrate
    gigavoxels: single object
    hybrid rasterized gigavoxels: too hard to render oit overlaps
    froxel refinement volumetric render: too slow
  engine 3: refinement renderer
    foxels: frustrum shaped voxels
    each pixel is one stack of foxels
    heirarchy all the things
    refine froxels around objects
    awesome data structure:
      foundation of frostbites new volumetric work
    year 2001 demo scene: everyone was doing radial blur w/ one bilinear tap, process pixels from center out
    compute shaders in concentric shells from light
      you do four lights in parallel so you don't notice the stalls
      "iterative diffusion due to repeated bilinear filter" ?
    only works for onscreen lights
    super happy, thought this was the end (just offscreen lights and perf issues)
    problem: too slow, 4x-10x depending on optimism, lots of unsolved problems (offscreen lights, thin feature aliasing)
      still cool direction!
    2+ years into project
  engine 4: splat engine
    "i hate splats" <- his prejudice
    evaluator runs in wavefronts of 4^3 voxels ?
    voxels, actually distance fields
    spawn one splat on screen per voxel ?
    groups of 256 points are grouped into clusters by sorting [ ] in hilbert order
      shoved into BVH
      use clusters to do vector quantization
      each cluster stores obb bounds for position and normals
      surface color is stored in dxtt atlas arranged so 16 nearby points fall in dxt tile (?)
      each point is then brt painted into 32 bits
        x 6 bits
        y 6 bits
        z 6 bits
        nx 3
        ny 3
        nz 3
    4.5 bytes per splat
    single pixel splats
    simon wrote an engine where it uses the atomic engine on ps4
      super happy not using the triangle hardware
    tried retro quads w/ textures
  final:
    scene cloud of objects
      ea object is cloud of points generated from distance fields
      when far away, just one pixel
      when close, each is renderer as another cloud of points (micro splats)
      in e3 trailer: were textured quads
    a cloud of point clouds of point clouds
    all levels and fine grain lod via point cloud decimation and precomputed prefiltered clouds at the last z levels
    for DOF: jitter by "circular confusion" ?
    edit list -- compute shader of doom --> SDF -> pre-filtered + clustered point clouds (3 levels) -- (atomic min64) --> ID/2 buffer 64bpp -> gbuffer (albedo, N, rough, metalness) ->
    used "imperfect shadowmaps"
      "great paper", 
      don't try good quality shadow maps, blast a bunch of little ones w/ hemispheric/parabaloid views
      you get shadows, you get god rays
      painterly style anyway
    added simple ambient occlusion
    discovered TAA
    voxelized the scene ? "tomorrow's children" ?
  motion blur was hard
    want each moment in time to have its own z buffer, the avg them ?
    does traditional post motion blur
  

Concepts to clarify:
  "Filtering"
    Pre-filtering ?
  mip-maps
  denoising
  prefix sum
  caches: where, how big, when it matters,
  CSG
  How does ray tracing hardware work??
    "blast"
    "t-lass" ?
    BVHs

Big question: how do user shaders work with Nanite?
  connection / handoff is confusing to me

"What are the most distracting *visual* artifacts in video games that bother you, personally, most of the time?"
  "Banding in subtle gradients. Just throw a simple dither in post it's free!"
    https://www.shadertoy.com/view/MdccD2

"Please stop using screen-space reflections [...]"
  ray tracing or cube map under the water

Half-edge data structures:
  http://15462.courses.cs.cmu.edu/fall2018/lecture/meshes/slide_028

Transparency:
  http://learnwebgl.brown37.net/11_advanced_rendering/alpha_blending.html
  http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-10-transparency/
  https://learnopengl.com/Advanced-OpenGL/Blending

What is: parallax mapping
  https://twitter.com/SebAaltonen/status/1593577976637038592
  terms: parallax mapping", QDM (xbox360 era), UGC, shell mapping (xbox1 era), Cone step mapping
  "If you need runtime compose with generate - I would still use it. Soft shadows in texture space are a nice boon too if composed to minimize sample count"
  "parallax mapping of any kind is a risk that needs to account for sliding textures and limited silhouettes; especially with UGC where you don't want people wasting time covering up the artefacts."
  https://developer.nvidia.com/gpugems/gpugems3/part-iii-rendering/chapter-18-relaxed-cone-stepping-relief-mapping
  https://www.gamedevs.org/uploads/quadtree-displacement-mapping-with-height-blending.pdf
    https://www.gdcvault.com/play/1012014/Quadtree-Displacement-Mapping-with-Height
    Michal Drobot
  https://www.unrealengine.com/marketplace/en-US/product/get-relief-rcsm-generator?sessionInvalidated=true
  
BotW lighting overview:
  https://www.youtube.com/watch?v=By7qcgaqGI4&t=2s
  more BoTW:
    cedec 2017

  great website for disecting old game rendering:
    https://noclip.website

  cell shading in Windwaker:
    https://www.youtube.com/watch?v=mnxs6CR6Zrk

  mario galaxy gravity and shadows:
    https://www.youtube.com/watch?v=QLH_0T_xv3I

Returnal Particles:
  https://twitter.com/SharmanJag/status/1607712952601219072
  3/ Basics. A particle size equals the voxel size. Particles request for vacant slots in a volume while moving. The voxels are represented with a single bit. 1 : "Occupied" and 0 : "Vacant". Requests are atomic operations. If "occupied", we resolve the collision, otherwise move.
  6/ Oh dear, the creature we created the effect for is not watertight. Well, I made a look-up table to map each vertex to a bone segment. All we needed to do was to populate each triangle with points and trace towards the bone while requesting for occupancy. Now we have our volume

More amazing Returnal grass:
  https://twitter.com/JankkilaRisto/status/1605905980528553984
  12/ Here's the final version of the effect again, with a split screen showing the particle structure. The first particle in the chain is given velocity towards the player head. We damp the velocity when the particle is within a given radius from the player.

"Learning Modern 3D Graphics Programming (2012)":
  https://paroj.github.io/gltut/
  https://news.ycombinator.com/item?id=34165295

WebGPU native samples:
  https://github.com/samdauwe/webgpu-native-examples

Vulkan guides:
  https://vkguide.dev
  https://vulkan-tutorial.com

FP16 in shaders?
  https://interplayoflight.wordpress.com/2022/12/30/experimenting-with-fp16-in-shaders/

Geometry node limits:
  from max: Can do lists but not lists of lists

https://usegpu.live:
  Use.GPU is a set of declarative, reactive WebGPU legos. Compose live graphs, layouts, meshes and shaders, on the fly.
  It has a built-in shader linker and binding generator, which means a lot of the tedium of common GPU programming is eliminated, without compromising on flexibility.
  https://usegpu.live/demo/geometry/lines
  https://gitlab.com/unconed/use.gpu/-/blob/master/packages/app/src/pages/geometry/lines.tsx
  by https://acko.net/

Text rendering w/ Slug:
  http://sluglibrary.com/

Rotate 2d sprite via 3 sheers:
  https://cohost.org/tomforsyth/post/891823-rotation-with-three
  https://www.ocf.berkeley.edu/~fricke/projects/israel/paeth/rotation_by_shearing.html
  shader: https://www.shadertoy.com/view/dlfXRM

Teardown voxel frame analysis:
  https://acko.net/blog/teardown-frame-teardown/
  deferred rendering,
    albedo rgb, normal xyz, 
    (roughness, reflectivity, metallic, emissive), 
    motion xy, water mask
    depth linear z (u16)
    z-buffer: inverse z (f32)
  draw call: 36 verts, 12 triangles cube
    not voxels, instead object bounding box
    objects are 3d volume textures w/ 1byte/voxel
  transparency: 50% screen-door effect
  custom "early-z":
    custom z buffer, copied out 8 times/frame
    8 batches, can only early-z against previous batches
  cables, ropes and wires single seperate draw call
  another draw: smoke particles
    CPU simulated
    "the renderer makes eager use of blue-noise based screen door transparency"
  final pass: map-wide water
    "The game also has dynamic foam ripples on the water, when swimming or driving a boat. For this, the last N ripples are stored and evaluated in the same water shader, expanding and fading out over time"
  volumetric level wide shadow map:
    "Unlike the regular voxel objects, this map is actually 1-bit. Each of its 8-bit texels stores 2×2×2 voxels. So it's actually a 3504×200×3000 voxel volume. Like the other 3D textures, this has 2 additional MIP levels to accelerate raytracing, but it has that additional "-1" MIP level inside the bits, which requires a custom loop to trace through it."
       CPU-rendered, updated via dedicated thread (probably)
  light:
    "As is typical in a deferred renderer, each source of light is drawn individually into a light buffer, affecting only the pixels within the light's volume"
    "As this is irradiance, it does not yet factor in the color of each surface."
      denoise: "This uses a spiral-shaped blur filter around each point, weighted by distance.
    "Interestingly, while it looks like a height-based fog which thins out by elevation, it is actually just based on vertical view direction. A clever trick, and a fair amount cheaper.
  "This is now the fourth time that temporal reprojection and blending was applied in one frame: this is no surprise, given how much stochastic sampling was used to produce the image in the first place."

Teardown author blog:
  https://blog.voxagon.se/2020/12/03/spraycan.html

Teardown guy:
  https://www.gdcvault.com/play/1020142/Physics-for-Game-Programmers-Sprinkle
  https://ubm-twvideo01.s3.amazonaws.com/o1/vault/GDC2014/Presentations/Gustafsson_Dennis_Sprinkle_Fluids.pdf

Graphics blogs to read:
  http://www.kecho.me
  http://www.pauliehughes.com

"NVIDIA Nsight Graphics", like RenderDoc:
  https://developer.nvidia.com/nsight-graphics

Another interesting blog:
  https://nothke.github.io

Optimizing PS vs VS:
  https://twitter.com/SebAaltonen/status/1618647617105465344
  count the "varyings"
  "Varyings are very expensive on mobile, especially for high triangle count content. VS needs to store them to memory and PS needs to load them from memory. Passing things though VS is never a win. Just load directly in PS from UBO."
  Me: "Noob graphics question: if a varying is marked for flat interpolation, does that still count against you?"
    Seb: "I am not 100% sure about Apple HW in this regard. On AMD hardware, you don't get the interpolation instructions, but you get a LDS load instead. Because the varyings are in LDS. Apple HW still needs to load it from memory however. Don't know how much you save with nointerp."
    LDS: "low discrepancy sequence" ?
      or "low density storage"?
    AH! "Local Data Share"

Optimizing GPU occupancy and resource usage with large thread groups:
  https://gpuopen.com/learn/optimizing-gpu-occupancy-resource-usage-large-thread-groups/
  Local Data Share

"Implementing a GPU-driven debug line renderer"
  https://www.gijskaerts.com/wordpress/?p=190
  https://twitter.com/BelgianRenderer/status/1451990908757372929

DX11 "printf":
  http://c0de517e.blogspot.com/2013/07/dx11-gpu-printf.html

Nanite SW rasterizer b/c of memory bandwidth?
  https://twitter.com/tom_forsyth/status/1399446806476386304
  "MeshShaders should give us the same capability to decompress and pass directly to the HWrast. But as far as I know Nanite doesn't currently use them - they were developed in parallel."

"Gpu Driven Text":
  https://jorenjoestar.github.io/post/gpu_driven_text/gpu_driven_text/

M1 gpu driver:
  https://asahilinux.org/2022/12/gpu-drivers-now-in-asahi-linux/

Nanite, Lumen, and virtual shadow maps in Fortnite:
  https://www.unrealengine.com/en-US/tech-blog/bringing-nanite-to-fortnite-battle-royale-in-chapter-4
    Looks like they opted to use geometry to define leaves instead of masked materials
    "we added new logic to the Nanite builder (an option called “Preserve Area” that can be enabled under mesh “Nanite Settings”) that redistributes the lost area to the remaining triangles by dilating out the open boundary edges."
    Nanite trees (~300-500k) vs before (~10-20k)
    "World Position logic now being evaluated inside of Nanite’s rasterizer"
    Moved wind into texture, read from that during World Position Offset eval in rasterizer   
      they use Houdini Vellum for wind sim
      lookup position + quat
    "Landscape" grass?
      https://docs.unrealengine.com/5.0/en-US/creating-landscapes-in-unreal-engine/
      "We used opaque materials with actual geometry for the blades of grass"
        like Ghost of T.
  https://www.unrealengine.com/en-US/tech-blog/virtual-shadow-maps-in-fortnite-battle-royale-chapter-4
  https://www.unrealengine.com/en-US/tech-blog/lumen-brings-real-time-global-illumination-to-fortnite-battle-royale-chapter-4
  plus more:
    https://www.unrealengine.com/en-US/blog/battle-testing-unreal-engine-5-1-s-new-features-on-fortnite-battle-royale-chapter-4
    Local Exposure, Temporal Super Resolution, Clouds,

What is Unreal's "World Position Offset" ? Used extensively in Fortnite for animation:
  https://docs.unrealengine.com/4.27/en-US/Resources/ContentExamples/MaterialNodes/1_10/
  oh this is just vertex shader stuff, just manipulating the output position
  Comes with this caveat:
  "When using World Position Offset to expand your object beyond its original bounds, keep in mind that the renderer still uses those original bounds. This means that you may see culling and shadowing errors. "
  this seems to imply they don't allow vertex position manipulation in custom materials for shadows, zdepth, culling etc. Makes sense.

How does Unreal's "SplineThicken" work?
  "The SplineThicken function serves as a way to make very thin polygons look thicker at render time. This is perfect for cables, hair, grass, and other such objects."

IMPORTANT: Using WebGPU from native C++:
  https://eliemichel.github.io/LearnWebGPU/

Look at matrices:
  https://blog.42yeah.is/rendering/2023/01/28/look-at.html

To read:
  http://advances.realtimerendering.com
  https://www.youtube.com/channel/UC9V4KS8ggGQe_Hfeg1OQrWw

Graphics reading list:
  https://interplayoflight.wordpress.com/2018/09/30/readings-on-the-state-of-the-art-in-rendering/

Rendering Engine Architecture Conference:
  https://enginearchitecture.realtimerendering.com
  https://www.youtube.com/channel/UCWhvzM3b8N_JbmmoM1fkeGg

WASM threads:
  https://www.youtube.com/watch?v=khM7iilBgc0
  https://medium.com/google-earth/performance-of-web-assembly-a-thread-on-threading-54f62fd50cf7

Coop-coep for SharedArrayBuffer, performance.measureUserAgentSpecificMemory():
  https://web.dev/coop-coep/
    "Use COOP and COEP to set up a cross-origin isolated environment and enable powerful features like SharedArrayBuffer, performance.measureUserAgentSpecificMemory() and high resolution timer with better precision."

Offscreen canvas:
  https://medium.com/samsung-internet-dev/offscreencanvas-workers-and-performance-3023ca15d7c7

WASM SIMD:
  https://v8.dev/features/simd
  https://www.infoq.com/articles/webassembly-simd-multithreading-performance-gains/

Paint like Miyazaki:
  http://ghiblicon.blogspot.com/2015/01/ghibli-museum-sketching-set-miyazaki.html
  https://animationobsessive.substack.com/p/how-to-paint-like-hayao-miyazaki

Ghost of T talk on wind, cloth, grass, particles:
  https://www.youtube.com/watch?v=d61_o4CGQd8

Skybox:
  https://learnopengl.com/Advanced-OpenGL/Cubemaps

What's a swapchain ?
  general name for double or tripple buffer

What is tiling? Deferred tiling?

Virtual Shadow Maps:
  https://www.unrealengine.com/en-US/tech-blog/virtual-shadow-maps-in-fortnite-battle-royale-chapter-4
  "Ray-traced shadows are not currently a good fit as they are not performant when forced to rebuild acceleration structures to account for the significant dynamic deformation and animation"
  Previously:
    There are several conventional shadow map cascades near the player.
    Beyond these, there is a mix of distance field shadows and a single “far” cascade that covers most,
    The player character gets a dedicated per-object shadow map
    Screen-space contact shadows are layered on top of all of this to try to recover some of the detail missed by the low-resolution techniques and fill in any missing shadows in the distance.
  two additional constraints heavily undercut the ability to cache shadow maps: 
    a significant amount of animated deformation
      —chiefly trees—
    a continuously moving sun.

Filament:
  https://google.github.io/filament/Filament.html

Cascaded shadow maps (CSM):
  https://learnopengl.com/Advanced-Lighting/Shadows/Shadow-Mapping
  https://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf
  https://alextardif.com/shadowmapping.html
  https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps

Unity URP:
  Async:
    volumetric, SSAO, SSR, 

Ambient occlusion using an object id texture?
  https://publications.lib.chalmers.se/records/fulltext/127825.pdf

A Sampling of Shadow Techniques:
  https://therealmjp.github.io/posts/shadow-maps/
  EVSM: "exponential variance shadow maps"
    https://www.martincap.io/project_detail.php?project_id=9

"Large Voxel Landscapes On Mobile":
  https://www.youtube.com/watch?v=LktNeVkofKU

Roblox lighting:
  https://youtu.be/fegbyALcnpI?t=2884
  basic: voxel lighting
    CPU, SIMD,
    sun is analytic, everything else is Light Propegation Volumes "LPV"
  Shadow maps:
    replaces voxel for sun
    cached (tiled) EVSM CSM
  Forward+:
    replaces all shadows w/ analytic
  Analytic lights use PBR:
    custom approx of GGX w/EC (energy conservation)
    Spherical Gaussian for low-end devices

Forward+:
  https://www.3dgep.com/forward-plus/#Forward
  https://takahiroharada.files.wordpress.com/2015/04/forward_plus.pdf

Depth Z non-linearity, linear Z, reverse Z:
  perspective matrices have Z proprotional to 1/z in clip space, so all precision is at front
  to linearize, u need to reverse what the perspective matrix did
  reverse Z: set the near plane to 1 and far plane to 0
    "the quasi-logarithmic distribution of floating-point somewhat cancels the 1/z nonlinearity"
  https://developer.nvidia.com/content/depth-precision-visualized
  https://learnopengl.com/Advanced-OpenGL/Depth-testing
    // assumes ndc is [-1,1]
    float ndc = depth * 2.0 - 1.0; 
    float linearDepth = (2.0 * near * far) / (far + near - ndc * (far - near));	

Vulkan's design issues: "The problem statement at the beginning of this document is an extremely well conceived and lucid analysis of Vulkan's design issues.":
  https://github.com/KhronosGroup/Vulkan-Docs/blob/main/proposals/VK_EXT_shader_object.adoc#problem-statement
  https://twitter.com/EricLengyel/status/1641866274404982784

"I want to talk about WebGPU":
  https://cohost.org/mcc/post/1406157-i-want-to-talk-about
  http://dryad.technology

Who is "Timothy Lottes"?
  Brian Karis (Nanite) asking about videos: https://twitter.com/BrianKaris/status/1653593003368546304
  Sebastian Aaltonen (HypeHyper,Ubisoft,Unity) soliciting opinion from: https://twitter.com/SebAaltonen/status/1653708606636802049
  tanh(mmalex) (Dream's guy) being effusive about him: https://twitter.com/mmalex/status/1653642891804266497
  https://www.mobygames.com/person/515834/timothy-lottes/
  https://news.ycombinator.com/item?id=5122674
  https://news.ycombinator.com/item?id=5120991
  https://schedule2019.gdconf.com/speaker/lottes-timothy.39755
    Timothy Lottes is an optimization-obsessed developer working as part of the AMD Graphics Performance R&D (GPRD) team, with prior experience at Epic, NVIDIA, Human Head Studios, and ILM. He is based in AMD Orlando working with the GPU hardware team, Vulkan driver team, and directly with game developers.
  https://www.gdcvault.com/play/1023512/Advanced-Graphics-Techniques-Tutorial-Day
    Timothy Lottes is an Engineer in the Developer Technology group at NVIDIA. Prior Timothy was a console game developer at Human Head Studios, in Systems Research and Development at Industrial Light and Magic, and owner/photographer/developer of Farrar Focus, a fine art landscape photography and digital photo development tools business.  

Kvark (rust wgpu guy):
  spri-v problems: http://kvark.github.io/spirv/2021/05/01/spirv-horrors.html

Text:
  https://faultlore.com/blah/text-hates-you/
  https://lord.io/text-editing-hates-you-too/

For "real" text, should probably use FreeType:
  https://freetype.org

Perf: batch upload all per-frame data!
  https://www.youtube.com/watch?v=m3bW8d4Brec&list=PLAOytOz0HZbJzO8xwDTCQA4cubNzzc2mH&index=5

Render Pipeline Shaders SDK:
  https://www.youtube.com/watch?v=PXMRUjs6SK8
  https://gpuopen.com/learn/rps_1_0/
  https://gpuopen.com/amd-at-gdc-2022/
  Activision is considering using this

Line rendering deep dive:
  https://panthavma.com/articles/lines/deep-overview-extraction/

Gaussian splatting
  https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
  https://poly.cam/gaussian-splatting
  https://huggingface.co/blog/gaussian-splatting
  https://aras-p.info/blog/2023/09/05/Gaussian-Splatting-is-pretty-cool/

  https://www.youtube.com/watch?v=VkIJbpdTujE

GPU driven rendering at anki3d:
  https://anki3d.org/gpu-driven-rendering-in-anki-a-high-level-overview/

"Linear Light, Gamma, and ACES" from a film vfx perspective:
  https://prolost.com/?tag=Tutorials#show-archive

Upscaling: nearest-neighbor, linear, cubic, bilinear, bicubic:
  https://medium.com/@amanrao032/image-upscaling-using-bicubic-interpolation-ddb37295df0
  https://en.wikipedia.org/wiki/Bicubic_interpolation

Reflections via "sparse voxel approach" ?
  "I am also starting to lean towards a sparse voxel approach over the SDF approach we used in Claybook. But let's see how it goes..."
  "voxel stuff via HDDA."
  https://twitter.com/SebAaltonen/status/1730293106178683322
  "Sponza, running Godot HDDA high quality real-time, open world, full Global Illumination (with full ambient and 
  rough reflections) at over 1080p, 30fps on.. A Radeon R7 240.. which  is almost _100 times slower_ than a 4090."
  https://twitter.com/reduzio/status/1730277740371206434
  Hierarchial Digital Differential Analyzer, 
  https://www.museth.org/Ken/Publications_files/Museth_SIG14.pdf
  https://www.youtube.com/watch?v=lvV79JQB_6M
  https://gist.github.com/reduz/10c3e5a4ee85474b7213b15fe563bc43

openvdb:
  C++ library for volumetric rendering stuff used by... everyone?
  https://www.openvdb.org

Shaders For People Who Don't Know How To Shader, a tutorial series for beginners!:
  https://github.com/Xibanya/ShaderTutorials

Great graphics vids:
  https://www.youtube.com/@simondev758
  https://store.steampowered.com/app/1270580/Mind_Over_Magic/
  https://github.com/simondevyoutube/Quick_Grass
  
RESTIR:
  https://interplayoflight.wordpress.com/2023/12/17/a-gentler-introduction-to-restir/
  https://twitter.com/h3r2tic/status/1488368086621343749?lang=en

Wgsl history explanation:
  https://news.ycombinator.com/item?id=38738820

Ray tracing from scratch:
  https://johanneshoff.com/raytrace-mirror/
  https://www.gabrielgambetta.com/computer-graphics-from-scratch/00-introduction.html
  https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-ray-tracing/how-does-it-work.html
  https://raytracing.github.io

  tinyraytracer:
    https://github.com/ssloy/tinyrenderer/wiki

"What do you get when you combine... ah never mind, I call this one "Potato Lumen" 😛
... because it uses a Lumen-style screen-space radiance cache, but aims to run on potato GPUs, tracing only 1 ray for every 16 pixels.
How do I feed it so sparsely? ReSTIR of course 🤣🫒🛢️":
  https://twitter.com/h3r2tic/status/1655494432496517121

How Video Games Work:
  https://www.youtube.com/watch?v=C8YtdC8mxTU

Introduction to Computer Graphics
  https://www.youtube.com/@cem_yuksel

lisyarus/Lisitsa Nikita's Web GPU demo (C++):
  https://github.com/lisyarus/webgpu-demo
    uses: https://github.com/gfx-rs/wgpu-native

"High Performance Voxel Engine: Vertex Pooling":
  https://nickmcd.me/2021/04/04/high-performance-voxel-engine/

Gouraud shading:
  shade at vertex, interp at frag

Texture mapping:
  https://youtu.be/hKHS08kXtDI?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=636

Tri-linear filtering:
  bilinear filter in two nearest mip-map levels,
  linear interp between those
  https://youtu.be/hKHS08kXtDI?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=2453
  anisotropic filtering: more accurate

jendrikillner's database of graphics articles:
  https://www.jendrikillner.com/article_database/

Mesh colors:
  https://youtu.be/bv7vS60qJxg?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=3109

printf from shader code:
  https://therealmjp.github.io/posts/hlsl-printf/

Environment mapping / skybox:
  only need direction (no pos) -> UV coord
  1. lat/long
  2. light-probe / angular
  3. vertical cross / cube map
  https://youtu.be/PeAvKApuAuA?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=749
  to transfer clip-space coordinates of full-screen quad render, use inverse view-projection matrix (3x3)
  render background last, saves computation
  could use single triangle 2x2

Reflection:
  sample from environment map in reflect(n, view_dir)
    (low mip map for blur?)
  flat mirror:
    https://youtu.be/h2RTBs1xl6w?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=2192
    render scene reflected through the portal that is the mirror
    reflect the camera's position
    environment reflections work fine w/ changed normals
    for object reflections, need to distort uv coord using frag_coord somehow
      also probably use alpha value to communicate if there's an object present or not
  cube map reflections:
    could render whole scene 6 times to new cube map
    if ur doing this next to a mirror, use new cube map for reflected scene
  to get infinite-ish reflections between two objects:
    swap who gets the new cube map,
    use the old one as the probe for the other object,
    ea frame u get another level of reflection 
  for more reflections:
    place cube maps throughout scene, hueristically, pick closest
    could render only 1 cube face per frame

Render Equation:
  Cem: https://www.youtube.com/watch?v=wawf7Am6xy0&list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&index=13
  defined for each point on each surface
  integral over hemisphere of: lighting_term(inDir) * geo_term(theta) * BRDF(inDir, outDir)
  Lout(outDir) = int_omega(Lin(inDir)*cos(theta)*BRDF(inDir, outDir)*delta_inDir)

Cem on shadows:
  https://youtu.be/C2Gn6oOxSu0?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=143
  lighting_term(inDir) = intensity(inDir) * visiblity(inDir)
  light attenuation: intensity is brighter near the light source
    for point light, attenuation is 1/d^2
  real-time area lights r possible 

Hmm can u use rtx hardware for gpu-side collision detection?
  ray vs scene..

Gaussian Splatting is pretty cool!:
  https://aras-p.info/blog/2023/09/05/Gaussian-Splatting-is-pretty-cool/

Perspective shadow maps:
  https://youtu.be/C2Gn6oOxSu0?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=2628
  compute shadows in canonical view volume
  seems not as good as cascaded shadow maps

Shadow volumes:
  https://youtu.be/C2Gn6oOxSu0?list=PLplnkTzzqsZS3R5DjmCQsqupu43oS9CFN&t=3459
  the downside is generating and dealing with the shadow geometry

stancil buffer: way of counting something per-pixel 🤯

Sand programs, rule based:
  https://www.youtube.com/watch?v=xvlsJ3FqNYU
  good fit for GPU?

How to approach rendering problems:
  http://c0de517e.blogspot.com/2023/02/how-to-render-it-ten-ideas-to-solve.html
  Use the right space.
  Data representation and its properties.
    Consider the three main phases of computation. (scene encoding, solver, and real-time retrieval)
    Consider the dual problem.
  Compute over time.
  Think about the limitations of available data.
    Machine learning as an upper limit.
  The hierarchy of ground truths.
  Use computers to help along the way.
  Humans over math.
  Find good priors.
  Delve deep.
  Shortcut via proxies.

"Machine Learning for Game Devs":
  https://www.ea.com/seed/news/machine-learning-game-devs-part-1

GPU sorting overview:
  https://linebender.org/wiki/gpu/sorting/

Life of a triangle:
  "there's this realy great post by nvidia" -simondev
  https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline
  "amd also has a really great talk"
  https://www.youtube.com/watch?v=Y2KG_4OxDBg

Humus graphics blog:
  https://www.humus.name

"filterable procedurals":
  https://iquilezles.org/articles/filterableprocedurals/

Phone wire AA:
  https://www.humus.name/index.php?page=3D&ID=89&source=post_page-----727f9278b9d8--------------------------------

TODO: how does tri-planar mapping work?

"Demystifying GPU Compute Architectures":
  https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures

"CLOSED-FORM IMPLICIT SURFACES ON THE GPU":
  https://www.mattkeeter.com/research/mpr-ornl-2021.pdf

WebGPU inspector:
  https://github.com/brendan-duncan/webgpu_inspector
  https://chromewebstore.google.com/detail/webgpu-inspector/holcbbnljhkpkjkhgkagjkhhpeochfal

"Building the DirectX shader compiler better than Microsoft?":
  https://devlog.hexops.com/2024/building-the-directx-shader-compiler-better-than-microsoft/
  "successor and descendant of WebGPU for native graphics"

mach engine project w/ zig:
  https://devlog.hexops.com/2024/mach-v0.3-released/

WebGPU impl on android vulkan:
  https://vulkan.org/user/pages/09.events/vulkanised-2024/vulkanised-2024-brandon-jones-google.pdf

Vulkan synchronization made easy be nice_byte:
  https://vulkan.org/user/pages/09.events/vulkanised-2024/vulkanised-2024-grigory-dzhavadyan.pdf

Digital Foundry on TAA:
  https://www.youtube.com/watch?v=WG8w9Yg5B3g

TimothyLottes on TAA:
  https://twitter.com/NOTimothyLottes/status/1756725714198360576
  "For TAA sharpness ... 4x4 lanczos filter can be used for reprojection with a clamp to the nearest 2x2 {min,max}"

To render giant view distances w/ high depth buffer precision:
  https://www.khronos.org/opengl/wiki/Depth_Buffer_Precision
  The typical approach is to use a multipass technique. The application might divide the geometry database into regions that don't interfere with each other in Z. The geometry in each region is then rendered, starting at the furthest region, with a clear of the depth buffer before each region is rendered. This way the precision of the entire depth buffer is made available to each region.

IQ's 2D SDFs:
  http://iquilezles.org/articles/distfunctions2d/
  pie seems very versatile: https://www.shadertoy.com/view/3l23RK

Capsule-based ambient occlusion:
  https://www.shadertoy.com/view/llGyzG
  https://github.com/Fewes/CapsuleOcclusion
  http://miciwan.com/SIGGRAPH2013/Lighting%20Technology%20of%20The%20Last%20Of%20Us.pdf
  https://knarkowicz.files.wordpress.com/2017/05/knarkowicz_rendering_sw2_dd_20171.pdf

Blur shader comparison:
  https://mini.gmshaders.com/p/blur-philosophy

Godot discussing render graph upgrades:
  https://godotengine.org/article/rendering-acyclic-graph/

how to think about gpu aglorthims:
  https://gpu-primitives-course.github.io
  https://gpu-primitives-course.github.io/sa-course-notes.pdf

"Always-Sharp SDF Textures":
  https://julhe.github.io/posts/always_sharp_sdf_textures/
  a generalized version of bgolus's "prestine grid" ?
    https://bgolus.medium.com/the-best-darn-grid-shader-yet-727f9278b9d8

Baking lighting into vertices:
  https://simonschreibt.de/gat/homeworld-2-backgrounds/

Work graphs:
  https://developer.nvidia.com/blog/work-graphs-in-direct3d-12-a-case-study-of-deferred-shading/
  https://devblogs.microsoft.com/directx/d3d12-work-graphs-preview/
  https://gpuopen.com/gpu-work-graphs-in-vulkan/
  https://gpuopen.com/gdc-2024-announce/
  https://gpuopen.com/learn/gpu-work-graphs/gpu-work-graphs-part1/
  https://gpuopen.com/learn/gdc-2024-workgraphs-drawcalls/
  https://gpuopen.com/events/amd-at-gdc-2024/
  
vulkan grass w/ instancing and indirect draw:
  https://www.thegeeko.me/blog/foliage-rendering

To do pixel art:
  start at 360p. 2x is 720, 3x is 1080, 4x is 1440 and 6x is 2160

Recreating Nanite:
  https://jglrxavpok.github.io/2024/01/19/recreating-nanite-lod-generation.html
  https://jglrxavpok.github.io/2024/03/12/recreating-nanite-faster-lod-generation.html

Font and vector art gen using mesh shaders:
  https://gpuopen.com/learn/mesh_shaders/mesh_shaders-font_and_vector_art_rendering_with_mesh_shaders/

Particle system:
  Niagara basics:
    https://www.youtube.com/watch?v=04k9JDx-KTM
    https://www.youtube.com/watch?v=JepePaqD7-Y&list=PLUi8nuTUEtTshYxpmR7brPE3tV7JsO0VP&index=2
      systems, emitters, modules ?
    Spawn: continuous, burst, spawn-per-unit (based on emitter travel?)

Grass w/ mesh shaders:
  https://gpuopen.com/learn/mesh_shaders/mesh_shaders-procedural_grass_rendering/?utm_source=twitter&utm_medium=social&utm_campaign=meshshaders

Indie dev's approach to a tree:
  https://youtu.be/q8WL1Z1W4Gs?t=238

Dom to canvas:
  https://robert.ocallahan.org/2011/11/drawing-dom-content-to-canvas.html
  https://medium.com/webmr/dom-to-canvas-for-webxr-in-just-7-years-e032060437fb

BabylonJS's "greased line":
  https://doc.babylonjs.com/features/featuresDeepDive/mesh/creation/param/greased_line
  
Toon shading in Hi Fi Rush
  https://gdcvault.com/play/1034330/3D-Toon-Rendering-in-Hi

Understanding BC texture compression format:
  https://learn.microsoft.com/en-us/windows/win32/direct3d10/d3d10-graphics-programming-guide-resources-block-compression
    "While lossy, block compression works well and is recommended for all textures that get 
    transformed and filtered by the pipeline. Textures that are directly mapped to the screen 
    (UI elements like icons and text) are not good choices for compression because artifacts are more noticeable."
  https://www.reedbeta.com/blog/understanding-bcn-texture-compression-formats/

Render filled bezier curve w/ single triangle:
  https://youtu.be/SO83KQuuZvg?t=1614

Nanite materials:
  https://www.unrealengine.com/en-US/blog/take-a-deep-dive-into-nanite-gpu-driven-materials

Path tracing, bvh in webgpu:
  https://www.vaultcg.com/blog/casually-raytracing-in-webgpu-part2/

Blue Noise summary by demofox:
  https://www.youtube.com/watch?v=tethAU66xaA&t=1s

WebGPU features report caniuse:
  https://webgpureport.org

GPU driven webgpu:
  ExectureIndirect: https://github.com/gpuweb/gpuweb/issues/431
  drawIndirect: https://github.com/gpuweb/gpuweb/issues/31
  bindless: https://github.com/gpuweb/gpuweb/issues/380

Good rendering equation review:
  https://chuckleplant.github.io/2017/05/28/light-shafts.html

Ambient occlusion overview:
  https://vr.arvilab.com/blog/ambient-occlusion

Getting started with Graphics Programming:
  A. Physcially Based Rendering: From Theory to Implementation (https://pbrt.org)
    first half or so. software path tracer
  B. DirectX-Graphics-Samples (https://github.com/microsoft/DirectX-Graphics-Samples)
    1. hello world https://github.com/microsoft/DirectX-Graphics-Samples/tree/master/Samples/Desktop/D3D12HelloWorld
    2. https://alextardif.com/DX12Tutorial.html
    3. MSFT MiniEngine, AMD’s Cauldron, NVIDIA’s Falcor
      https://github.com/microsoft/DirectX-Graphics-Samples/tree/master/MiniEngine
      https://gpuopen.com/cauldron-framework/
      https://developer.nvidia.com/falcor
  project ideas:
    Efficient scene submission, Material and lighting models, Shadows, Color treatment, Streaming, 
      Indirect lighting, Volumetrics, Anti-aliasing
  "The general recommendation is to develop a project or demo that you can show off."
  
Logarithmic depth buffer:
  https://threejs.org/examples/?q=dept#webgl_camera_logarithmicdepthbuffer