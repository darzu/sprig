Critique of TSC:
    "Their code makes pretty heavy use of megamorphic object shapes and unnecessary 
    dynamic property accesses (both well-known JavaScript speed bumps)"
        https://esbuild.github.io/faq/#why-is-esbuild-fast
        https://mrale.ph/blog/2015/01/11/whats-up-with-monomorphism.html
        https://github.com/microsoft/TypeScript/issues/39247

Rant about JS:
    https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f
    
GPU memory access costs:
  https://computergraphics.stackexchange.com/questions/37/what-is-the-cost-of-changing-state
    TODO: read for good GPU perf tips
  most to least expensive state changes:
    render target (~60K/s)
    program (~300K/s)
    ROP
    texture bindings (~1.5M/s)
    vertex format
    UBO bindings
    vertex bindings
    uniform updates (~10M/s)

On latency (via Redblob):
    https://pavelfatin.com/typing-with-pleasure/
    https://www.youtube.com/watch?v=vOvQCPLkPt4
    https://www.inkandswitch.com/slow-software.html

Efficient code is harder to change, from redblob lementing mapgen4:
    https://twitter.com/redblobgames/status/1362852520096198656

Visualizing perf:
    https://dubroy.com/blog/visualizing-packrat-parsing/

Animometer Test
    On Mac M1, WebGPU, soft limit of ~85,000 triangle to stay at 60fps
    http://austin-eng.com/webgpu-samples/samples/animometer
    "renderBundles" is essential
    "dynamicOffsets" has no impact (so far)

For automatic lod'ing:
    Maybe convert to volumetric first, then use marching cubes (or square voxels) to create lower detail versions?

GPU architectures:
    https://rastergrid.com/blog/gpu-tech/2021/07/gpu-architecture-types-explained/

Debugging html5 games:
    https://www.html5rocks.com/en/tutorials/games/abouttracing/
    chrome://tracing/
    console.time("update"),
    console.timeEnd("update");

JS obj polymorphism is bad:
    https://mrale.ph/blog/2015/01/11/whats-up-with-monomorphism.html

Elements kinds in V8: https://v8.dev/blog/elements-kinds
    - SMI is fastest:
        The range for Smis on 64-bit platforms is -2^31 to 2^31-1 (2³¹≈ 2*10⁹)
        -2147483648 to 2147483647
    - Avoid reading beyond the length of the array
    - for-of, .forEach, and iterative for loop -- all on par
    - Avoid elements kind transitions
    - Prefer arrays over array-like objects
    - Avoid polymorphism
        https://v8.dev/blog/elements-kinds#avoid-polymorphism
        if a function only ever takes one element kind, it's faster
        built-ins like Array.prototype.forEach are much more efficient at this
    - Avoid creating holes

    Kinds:
         // The "fast" kind for elements that only contain SMI values. Must be first
        // to make it possible to efficiently check maps for this kind.
        PACKED_SMI_ELEMENTS,
        HOLEY_SMI_ELEMENTS,

        // The "fast" kind for tagged values. Must be second to make it possible to
        // efficiently check maps for this and the PACKED_SMI_ELEMENTS kind
        // together at once.
        PACKED_ELEMENTS,
        HOLEY_ELEMENTS,

        // The "fast" kind for unwrapped, non-tagged double values.
        PACKED_DOUBLE_ELEMENTS,
        HOLEY_DOUBLE_ELEMENTS,

        // The "slow" kind.
        DICTIONARY_ELEMENTS,

        // Elements kind of the "arguments" object (only in sloppy mode).
        FAST_SLOPPY_ARGUMENTS_ELEMENTS,
        SLOW_SLOPPY_ARGUMENTS_ELEMENTS,

        // For string wrapper objects ("new String('...')"), the string's characters
        // are overlaid onto a regular elements backing store.
        FAST_STRING_WRAPPER_ELEMENTS,
        SLOW_STRING_WRAPPER_ELEMENTS,

        // Fixed typed arrays.
        UINT8_ELEMENTS,
        INT8_ELEMENTS,
        UINT16_ELEMENTS,
        INT16_ELEMENTS,
        UINT32_ELEMENTS,
        INT32_ELEMENTS,
        FLOAT32_ELEMENTS,
        FLOAT64_ELEMENTS,
        UINT8_CLAMPED_ELEMENTS,

Check for more perf tips:
    https://v8.dev/blog

In a big world with float problems: subtact before multiply
    https://youtu.be/Ur53sJdS8rQ?t=812

vec3 on the pass-by-value?
  It's a huge PITA to have vec3's be pass-by-reference
  Can we pack 3 floats into some JS object that's pass by value?
    Maybe 2 float32s into one of JS's float 64s

  https://v8.dev/blog/bigint
  bigints are "arbitrary precision" and pass by value. 
    And you can bitwise operate on them.
    So we should be able to pack 3 float32s into a bigint.
    Unsure about perf on this, probably horendous
    bad bitwise perf when negative
    new Float64Array(BigInt64Array.of(1n).buffer)[0]

Big speed ups by porting to rust?
  Maybe not: https://zaplib.com/docs/blog_post_mortem.html

Proposal for JS tuples & records:
  https://github.com/tc39/proposal-record-tuple
  https://bugzilla.mozilla.org/show_bug.cgi?id=1658309

Measure WebGPU perf?
  https://gpuweb.github.io/gpuweb/#timestamp

Regarding window compositors (they r evil?):
  https://raphlinus.github.io/ui/graphics/2020/09/13/compositor-is-evil.html

rust vs js:
  https://github.com/dmaynard/chaos-screen-saver/blob/master/README.md
  tried and failed to make biz of js->rust:
    https://zaplib.com/docs/blog_post_mortem.html
  https://news.ycombinator.com/item?id=32098016
    "It's hard to beat JavaScript because the VMs are amazing. If you know a few tricks, like how to let the VM know you want integer math [1], you can get performance that's not too far off from native C in many cases. If you have a JavaScript application with a few hot functions that are slow, optimizing the JavaScript usually makes more sense than reaching for WASM."
      "In my experience the place wasm really shines is in implementing custom data structures. A wasm b-tree, skip list, rope, etc seems to outperform the equivalent javascript code by many times."
  
asm.js
  http://asmjs.org/spec/latest/
    specifies things like "|0"

perf gains by using int math in JS:
  https://james.darpinian.com/blog/integer-math-in-javascript
  
Details of D3D12 memory pools:
  https://therealmjp.github.io/posts/gpu-memory-pool/

Pipeline GPU stats from Doug:
  shadowPipeline, 0.162398ms,
  triRender, 1.012784ms,
  outlineRender, 2.091969ms,
  renderStars, 0.120638ms,
  postProcess, 2.823797ms,

Animometer:
  WebGL: 
    https://github.com/kenrussell/webgl-animometer
    http://kenrussell.github.io/webgl-animometer/Animometer/tests/3d/webgl.html
    http://sprig.land/webgl-animometer/Animometer/tests/3d/webgl.html
    https://sprig.land/webgl-animometer/Animometer/tests/3d/webgl.html?webgl_version=2&use_ubos=1&use_multi_draw=1
    ?webgl_version=2&use_ubos=1&use_multi_draw=1

Webgpu timestamp querries:
  https://github.com/OmarShehata/webgpu-compute-rasterizer/blob/main/how-to-use-timestamp-queries.md

New JS stuff to consider:
  FinalizationRegistry
  WeakRef
  Object.prototype.hasOwnProperty
  Error.cause
  "at" method?
  Object.hasOwn

Current memory/frame:
  ~341kb

Search for:
  TODO(@darzu): perf

"Shader functions I'm fairly certain I'll never want to use":
  https://twitter.com/m_ninepoints/status/1578622643128803328
  lit, reflect/refract, sinh/cosh/tanh, degrees, log10, faceforward
  "I generally agree with the sentiment that, trig and inv-trig functions in a shader are often a code smell (or maybe an algebra smell?) and they can often be simplified and removed with some identities, etc. But there are occasional legit uses for them." @reedbeta

With GPUs and standard forward rendering,
  why isn't there more frame-to-frame re-use?
  e.g. my vertices and pixel calculations are mostly the same answer,
    why should we do all the work each frame?
  maybe vertex shader outputs could be re-used and just readjusted slightly based on camera change and moving objects

about://tracing
  https://stackoverflow.com/questions/56329780/inexplicit-task-in-chrome-perfomance-devtools/74075943#74075943
  about:tracing can be very overwhelming, and it's difficult to document because of the size and because what's in there changes when we make code changes. So the best documentation is the code itself. There might be other documentation but unfortunately I don't know where it is exactly. This looks like the right place to start: https://sites.google.com/a/chromium.org/dev/developers/how-tos/trace-event-profiling-tool

Perf stats on Chrome:
  open -a 'Google Chrome Canary' --args --disable-dawn-features=disallow_unsafe_apis
  
To learn: occlusion queries
  ChatGPT: An occlusion query works by rendering an object's axis-aligned bounding box to the depth buffer and then checking to see if any pixels in the bounding box passed the depth test.
  https://developer.nvidia.com/gpugems/gpugems2/part-i-geometric-complexity/chapter-6-hardware-occlusion-queries-made-useful
  WebGPU:
    https://www.w3.org/TR/webgpu/#occlusion

Memory Caches, CPU:
  What is a cache line?
    probably ~64 bytes, it's the smallest block of data that can be stored in your CPUs L1-L3 caches

  https://akkadia.org/drepper/cpumemory.pdf (2007)
    https://stackoverflow.com/questions/8126311/how-much-of-what-every-programmer-should-know-about-memory-is-still-valid
    https://news.ycombinator.com/item?id=19302299
    https://news.ycombinator.com/item?id=25908018
    https://news.ycombinator.com/item?id=3919429
  CppCon 2014: Mike Acton "Data-Oriented Design and C++"
    https://www.youtube.com/watch?v=rX0ItVEVjHc&t=4681s
      lie 1: software is the platform
        truth: hardware is the platform
      lie 2: world modeling
        engineering by analogy or story telling; doesn't match reality
        truth: design around the data, not an idealized world
      lie 3: code is more important than the data
        truth: your main responsibility is to transform data, solve that first, not the code design
    purpose of any code is to transform data
      programmer is fundamentally responsible for the data, for ensuring the transform happens correctly
      programmer's job is not to write code, programmer's job is to solve (data transformation) problems
    L2 cache misses are the most significant time sink
    tip to determine information density: print it out and zip it
      e.g. "do_spawn" bool variable
    when u have poor cache line utilization for a decision per frame:
      a) make the decision many more times
      b) combine with other reads that will statistically be needed (cannot solve this in an abstract bubble)
      c) look over frames, only read when needed (?)
        so precalculate when the rare event will be needed and queue that for the future
    (?) bools mean lots of last minute decision making

  "Data-Oriented Design Principles":
    https://data-oriented.design
    The purpose of all programs, and all parts of those programs, is to transform data from one form to another.
    If you don’t understand the data you don’t understand the problem.
    Conversely, understand the problem by understanding the data.
    Different problems require different solutions.
    If you have different data, you have a different problem.
    If you don’t understand the cost of solving the problem, you don’t understand the problem.
    If you don’t understand the hardware, you can’t reason about the cost of solving the problem.
    Everything is a data problem. Including usability, maintenance, debug-ability, etc. Everything.
    Solving problems you probably don’t have creates more problems you definitely do.
    Latency and throughput are only the same in sequential systems.

  https://agner.org/optimize/


Unity DOTs:
  https://www.youtube.com/watch?v=u8B3j8rqYMw
    "the global energy required to transform some data should be proportional to the amount of surprise."
    two types of abstraction: utility abstraction (good) vs story abstraction (bad)
    if you don't understand your data, you don't understand your problem
      Daryl: what is your data, what are your access patterns (transformations you need to do)
        that is your problem, your solution should mirror that
    performant by default, optimizable by default, concurrent by default

What is:
  icache
    virtual function stuff
  shared memory modes,
    properties:
      GPU-visible,
      cached,
      GPU Coherent,

    Heap-cacheable
    Heap-write-combined
    Physical-uncached
    GPU-write-combined
    GPU-write-combined-read-only
    GPU-cacheable
    GPU-cacheable-noncoherent-RO
    Command-write-combined
    Command-cacheable

"":
  https://www.forrestthewoods.com/blog/should-small-rust-structs-be-passed-by-copy-or-by-borrow/
  disassembly via GodBolt
    https://godbolt.org/z/1gENA_
  branch prediction via VTune
    https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.lshm3t

FinalizationRegistry:
  "A FinalizationRegistry object lets you request a callback when an object is garbage-collected."
  This might let us track non-garbage collected objects!
  
"Memory in Javascript— Beyond Leaks":
  https://medium.com/walkme-engineering/memory-in-javascript-beyond-leaks-8c1d697c655c
  eh, nothing new here.

Measuring cache misses?
  https://perf.wiki.kernel.org/index.php/Tutorial#Counting_with_perf_stat
  https://www.quora.com/Whats-the-easiest-way-to-profile-my-application-to-see-the-CPU-cache-L1-L2-hit-miss-ratio-on-Linux
  https://stackoverflow.com/questions/34325735/how-to-measure-l1-l2-l3-cache-hits-misses-in-osx
  https://developers.redhat.com/blog/2014/03/10/determining-whether-an-application-has-poor-cache-performance-2
    Performance Monitoring Units "PMU" hardware
    https://en.wikipedia.org/wiki/Hardware_performance_counter

Apple GPU perf counters:
  https://developer.apple.com/documentation/metal/performance_tuning/optimizing_performance_with_gpu_counters

Optimizing for Apple Silicon:
  https://developer.apple.com/documentation/apple-silicon/tuning-your-code-s-performance-for-apple-silicon

Apple Instruments wwdc vids:
  https://developer.apple.com/videos/play/wwdc2019/411/
  https://developer.apple.com/videos/play/wwdc2018/410/
  https://developer.apple.com/videos/play/wwdc2022/10106/
  https://developer.apple.com/videos/play/wwdc2021/10211/

"Go Bindless:"
  https://developer.apple.com/videos/play/wwdc2022/10101

Chrome Alloc profiler tool:
  https://developer.chrome.com/docs/devtools/memory-problems/allocation-profiler/

V8 dev:
  https://stackoverflow.com/users/6036428/jmrk

Allocator:
  https://twitter.com/SebAaltonen/status/1616771875413049344
  LZCNT: Count the Number of Leading Zero Bits
  POPCNT: 

TBDR:
  "I allow next render pass to run vertex shaders before the previous pass finishes. This is the biggest optimization you want on mobile TBDR GPUs."
  https://twitter.com/SebAaltonen/status/1617488998825447424
  tile-based deferred rendering?
  https://developer.apple.com/documentation/metal/tailor_your_apps_for_apple_gpus_and_tile-based_deferred_rendering

Move vector operations onto the stack:
  1. we remove all out params from all vec/mat stuff, then all vec computations will use temp vecs
  2. you have to use vec3.copy(myLongtermVec, tempVec) to save anything
  3. then since we'll know there there isn't any vec reuse happening, we can inline all those vec operations at build time into just scalar stuff

  Added bonus that the code will all look cleaner, with the exception of the copy().
  during normal dev, you don't do the build step, everything runs fine.

Chrome flags:
  --gpu-launcher

  https://peter.sh/experiments/chromium-command-line-switches/
  https://chromium.googlesource.com/chromium/src/+/main/docs/gpu/debugging_gpu_related_code.md#debugging-in-the-gpu-process

Dawn + Node:
  https://dawn.googlesource.com/dawn/+/refs/heads/chromium/4959/src/dawn/node/

2/8/23, desktop 2080ti:
  js:1.21ms sim:1.21ms broad:(0.2ms o:34 e:69 c:0) 
  fps:36.6 tris:150536 verts:93979 dropped:0 entities:225 skew: ping: 
  WebGPU pipelines: 
    shadowPipeline0 0.01, triRender 0.36, oceanRender 28.94, skyPipeline 0.17, 
    outlineRender 0.26, postProcess 0.08, composeViews_0x0 0.02, composeViews_1x0 0.02, 
    composeViews_0x1 0.02, composeViews_1x1 0.01

Attaching renderdoc:
  set RENDERDOC_HOOK_EGL=0
  C:\Users\darzu>"C:\Users\darzu\AppData\Local\Google\Chrome SxS\Application\chrome.exe" --disable-dawn-features=disallow_unsafe_apis --disable-gpu-sandbox --gpu-startup-dialog

Beyond Performance: Introducing NVIDIA's New Graphics Debugger
  https://www.youtube.com/watch?v=X1-qi2w4cMs

The Peak-Performance-Percentage Analysis Method for Optimizing Any GPU Workload
  https://developer.nvidia.com/blog/the-peak-performance-analysis-method-for-optimizing-any-gpu-workload/

Gpu perf opt:
  (2012) https://on-demand.gputechconf.com/gtc/2012/presentations/S0514-GTC2012-GPU-Performance-Analysis.pdf
  (2020) https://developer.nvidia.com/siggraph/2020/video/sigg01

Debugger vs optimizer:
  RenderDoc better for debugging,
  Nsight, xcode instruments -> better for optimizing b/c it uses proprietary stuff

List of GPU perf optimization resources:
  https://gist.github.com/silvesthu/505cf0cbf284bb4b971f6834b8fec93d

Async compute deep dive:
  https://gpuopen.com/wp-content/uploads/2017/03/GDC2017-Asynchronous-Compute-Deep-Dive.pdf
  https://www.gdcvault.com/play/1024385/Advanced-Graphics-Tech-Async-Compute

D3D12 and Vulkan done right:
  https://gpuopen.com/wp-content/uploads/2017/03/GDC2017-D3D12-And-Vulkan-Done-Right.pdf
  https://www.gdcvault.com/play/1024732/Advanced-Graphics-Tech-D3D12-and

Render Hell:
  https://simonschreibt.de/gat/renderhell/

Latency numbers every programmer should know:
  https://gist.github.com/hellerbarde/2843375

GPU architecture resources:
  https://interplayoflight.wordpress.com/2020/05/09/gpu-architecture-resources/
  how nvidia gpus work: https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline
  how amd gpus work: https://gpuopen.com/presentations/2019/nordic-game-2019-triangles-are-precious.pdf

PIX + Chrome:
  https://gist.github.com/Popov72/41f71cbf8d55f2cb8cae93f439eee347

deep clone:
  structuredClone()
  https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm

Transfer array buffers between JS threads:
  https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Transferable_objects

Love and Lua: vectors?
  Apparently Love's Lua has the same non-stack restriction for vectors as JS
  https://love2d.org/forums/viewtopic.php?t=82815
    https://www.youtube.com/watch?v=wTjyM7d7_YA#t=23m6s
    https://love2d.org/forums/viewtopic.php?f=3&t=81457&start=50#p193807
  "You pretty much have to choose between arithmetic operator overloads or no-garbage vectors (e.g. CPML has no-garbage APIs, and then the arithmetic operators which do generate garbage).
  I did experiment with the gc metamethod and a pool of FFI vectors, but it ended up being slower than just plain FFI structs of doubles (which still generates garbage)."
    Lua math library: https://github.com/excessive/cpml
    https://love2d.org/forums/viewtopic.php?f=5&t=82770
  Lua has "metatables":
    https://www.lua.org/pil/13.html

Roblox uses "Luau":
  https://luau-lang.org/performance#native-vector-math
  "[we] provide a native value type that can store a 32-bit floating point vector with 3 components", "which essentially means we have native 3-wide SIMD support"

Sometimes use "var" instead of "let"/"const":
  https://github.com/microsoft/TypeScript/issues/52924
  https://github.com/microsoft/TypeScript/pull/52832

TODO: read all the asm.js material
  https://blog.mozilla.org/luke/2013/03/21/asm-js-in-firefox-nightly/
  https://brendaneich.com/2015/06/from-asm-js-to-webassembly/

Chrome on mac:
  open -a "Google Chrome Canary" --args --enable-dawn-features=allow_unsafe_apis